{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93395f57",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.rc('font', size=14)\n",
    "plt.rc('axes', labelsize=14, titlesize=14)\n",
    "plt.rc('legend', fontsize=14)\n",
    "plt.rc('xtick', labelsize=10)\n",
    "plt.rc('ytick', labelsize=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1142d044",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "IMAGES_PATH = Path() / \"images\" / \"dim_reduction\"\n",
    "IMAGES_PATH.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def save_fig(fig_id, tight_layout=True, fig_extension=\"png\", resolution=300):\n",
    "    path = IMAGES_PATH / f\"{fig_id}.{fig_extension}\"\n",
    "    if tight_layout:\n",
    "        plt.tight_layout()\n",
    "    plt.savefig(path, format=fig_extension, dpi=resolution)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74c72a55",
   "metadata": {},
   "source": [
    "# Dimensionality Reduction\n",
    "\n",
    "In some cases, reducing dimensionality of the training data may filter out some noise and unnecessary dteails and thus result in higher performance, but in general it won't, it will just speed up training.\n",
    "\n",
    "In higher dimensions, the average distance between 2 points will become larger as the number of dimensions increases, resulting in training instances that are likely to be far away from each other and new instances being far away from training instances, resulting in less reliable predictions. This also means that the more dimensions the training set has, the greater the risk of overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccfcf953",
   "metadata": {},
   "source": [
    "## Main Approahces for Dimenstionality Reduction\n",
    "\n",
    "- Projection <br>\n",
    "In most real-world problems, training instances are not spread out uniformly across all dimensions, and many features are either almost constant, or highly correlated. As a result, all training instances lie within (or close to) a much lower dimenstional subspace of the high dimensional space.\n",
    "\n",
    "\n",
    "- Manifold Learning <br>\n",
    "For some data, the subspace may twist and turn (e.g. famous Swiss roll toy dataset), and in this case, simply projecting onto a plane would squash different layers of the Swiss roll together. Hence what we want to do instead is to unroll the Swiss roll to obtain the 2D dataset where the different classes do not intersect with each other.\n",
    "\n",
    "A d-dimensional manifold is part of an n-dimensional space (where d < n). \n",
    "\n",
    "Manifold learning is a dimensionality reduction algorithm that works by modeling the manifold on which the training instances lie. This relies on the manifold assumption (*manifold hypothesis*) which holds that most real-world high-dimensional datasets lie close to a much lower-dimensional manifold.\n",
    "\n",
    "Another implicit assumption is that the task at hand will be simpler if expressed in the lower-dimensional space of the manifold. However, this assumption does not ALWAYS hold.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f34d393",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extra code\n",
    "\n",
    "import numpy as np\n",
    "from scipy.spatial.transform import Rotation\n",
    "\n",
    "m = 60\n",
    "X = np.zeros((m, 3))  # initialize 3D dataset\n",
    "np.random.seed(42)\n",
    "angles = (np.random.rand(m) ** 3 + 0.5) * 2 * np.pi  # uneven distribution\n",
    "X[:, 0], X[:, 1] = np.cos(angles), np.sin(angles) * 0.5  # oval\n",
    "X += 0.28 * np.random.randn(m, 3)  # add more noise\n",
    "X = Rotation.from_rotvec([np.pi / 29, -np.pi / 20, np.pi / 4]).apply(X)\n",
    "X += [0.2, 0, 0.2]  # shift a bit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dad689d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extra code – this cell generates and saves Figure 8–2\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "X2D = pca.fit_transform(X)  # dataset reduced to 2D\n",
    "X3D_inv = pca.inverse_transform(X2D)  # 3D position of the projected samples\n",
    "X_centered = X - X.mean(axis=0)\n",
    "U, s, Vt = np.linalg.svd(X_centered)\n",
    "\n",
    "axes = [-1.4, 1.4, -1.4, 1.4, -1.1, 1.1]\n",
    "x1, x2 = np.meshgrid(np.linspace(axes[0], axes[1], 10),\n",
    "                     np.linspace(axes[2], axes[3], 10))\n",
    "w1, w2 = np.linalg.solve(Vt[:2, :2], Vt[:2, 2])  # projection plane coefs\n",
    "z = w1 * (x1 - pca.mean_[0]) + w2 * (x2 - pca.mean_[1]) - pca.mean_[2]  # plane\n",
    "X3D_above = X[X[:, 2] >= X3D_inv[:, 2]]  # samples above plane\n",
    "X3D_below = X[X[:, 2] < X3D_inv[:, 2]]  # samples below plane\n",
    "\n",
    "fig = plt.figure(figsize=(9, 9))\n",
    "ax = fig.add_subplot(111, projection=\"3d\")\n",
    "\n",
    "# plot samples and projection lines below plane first\n",
    "ax.plot(X3D_below[:, 0], X3D_below[:, 1], X3D_below[:, 2], \"ro\", alpha=0.3)\n",
    "for i in range(m):\n",
    "    if X[i, 2] < X3D_inv[i, 2]:\n",
    "        ax.plot([X[i][0], X3D_inv[i][0]],\n",
    "                [X[i][1], X3D_inv[i][1]],\n",
    "                [X[i][2], X3D_inv[i][2]], \":\", color=\"#F88\")\n",
    "\n",
    "ax.plot_surface(x1, x2, z, alpha=0.1, color=\"b\")  # projection plane\n",
    "ax.plot(X3D_inv[:, 0], X3D_inv[:, 1], X3D_inv[:, 2], \"b+\")  # projected samples\n",
    "ax.plot(X3D_inv[:, 0], X3D_inv[:, 1], X3D_inv[:, 2], \"b.\")\n",
    "\n",
    "# now plot projection lines and samples above plane\n",
    "for i in range(m):\n",
    "    if X[i, 2] >= X3D_inv[i, 2]:\n",
    "        ax.plot([X[i][0], X3D_inv[i][0]],\n",
    "                [X[i][1], X3D_inv[i][1]],\n",
    "                [X[i][2], X3D_inv[i][2]], \"r--\")\n",
    "\n",
    "ax.plot(X3D_above[:, 0], X3D_above[:, 1], X3D_above[:, 2], \"ro\")\n",
    "\n",
    "def set_xyz_axes(ax, axes):\n",
    "    ax.xaxis.set_rotate_label(False)\n",
    "    ax.yaxis.set_rotate_label(False)\n",
    "    ax.zaxis.set_rotate_label(False)\n",
    "    ax.set_xlabel(\"$x_1$\", labelpad=8, rotation=0)\n",
    "    ax.set_ylabel(\"$x_2$\", labelpad=8, rotation=0)\n",
    "    ax.set_zlabel(\"$x_3$\", labelpad=8, rotation=0)\n",
    "    ax.set_xlim(axes[0:2])\n",
    "    ax.set_ylim(axes[2:4])\n",
    "    ax.set_zlim(axes[4:6])\n",
    "\n",
    "set_xyz_axes(ax, axes)\n",
    "ax.set_zticks([-1, -0.5, 0, 0.5, 1])\n",
    "\n",
    "save_fig(\"dataset_3d_plot\", tight_layout=False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2659d500",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_swiss_roll\n",
    "\n",
    "X_swiss, t = make_swiss_roll(n_samples=1000, noise=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "219096bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extra code – this cell generates and saves Figure 8–4\n",
    "import numpy as np\n",
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "darker_hot = ListedColormap(plt.cm.hot(np.linspace(0, 0.8, 256)))\n",
    "\n",
    "axes = [-11.5, 14, -2, 23, -12, 15]\n",
    "\n",
    "fig = plt.figure(figsize=(6, 5))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "ax.scatter(X_swiss[:, 0], X_swiss[:, 1], X_swiss[:, 2], c=t, cmap=darker_hot)\n",
    "ax.view_init(10, -70)\n",
    "set_xyz_axes(ax, axes)\n",
    "save_fig(\"swiss_roll_plot\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d0b140e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extra code – this cell generates and saves plots for Figure 8–5\n",
    "\n",
    "plt.figure(figsize=(10, 4))\n",
    "\n",
    "plt.subplot(121)\n",
    "plt.scatter(X_swiss[:, 0], X_swiss[:, 1], c=t, cmap=darker_hot)\n",
    "plt.axis(axes[:4])\n",
    "plt.xlabel(\"$x_1$\")\n",
    "plt.ylabel(\"$x_2$\", labelpad=10, rotation=0)\n",
    "plt.grid(True)\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.scatter(t, X_swiss[:, 1], c=t, cmap=darker_hot)\n",
    "plt.axis([4, 14.8, axes[2], axes[3]])\n",
    "plt.xlabel(\"$z_1$\")\n",
    "plt.grid(True)\n",
    "\n",
    "save_fig(\"squished_swiss_roll_plot\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe2febd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extra code – this cell generates and saves plots for Figure 8–6\n",
    "    \n",
    "axes = [-11.5, 14, -2, 23, -12, 15]\n",
    "x2s = np.linspace(axes[2], axes[3], 10)\n",
    "x3s = np.linspace(axes[4], axes[5], 10)\n",
    "x2, x3 = np.meshgrid(x2s, x3s)\n",
    "\n",
    "positive_class = X_swiss[:, 0] > 5\n",
    "X_pos = X_swiss[positive_class]\n",
    "X_neg = X_swiss[~positive_class]\n",
    "\n",
    "fig = plt.figure(figsize=(6, 5))\n",
    "ax = plt.subplot(1, 1, 1, projection='3d')\n",
    "ax.view_init(10, -70)\n",
    "ax.plot(X_neg[:, 0], X_neg[:, 1], X_neg[:, 2], \"y^\")\n",
    "ax.plot_wireframe(5, x2, x3, alpha=0.5)\n",
    "ax.plot(X_pos[:, 0], X_pos[:, 1], X_pos[:, 2], \"gs\")\n",
    "set_xyz_axes(ax, axes)\n",
    "save_fig(\"manifold_decision_boundary_plot1\")\n",
    "plt.show()\n",
    "\n",
    "fig = plt.figure(figsize=(5, 4))\n",
    "ax = plt.subplot(1, 1, 1)\n",
    "ax.plot(t[positive_class], X_swiss[positive_class, 1], \"gs\")\n",
    "ax.plot(t[~positive_class], X_swiss[~positive_class, 1], \"y^\")\n",
    "ax.axis([4, 15, axes[2], axes[3]])\n",
    "ax.set_xlabel(\"$z_1$\")\n",
    "ax.set_ylabel(\"$z_2$\", rotation=0, labelpad=8)\n",
    "ax.grid(True)\n",
    "save_fig(\"manifold_decision_boundary_plot2\")\n",
    "plt.show()\n",
    "\n",
    "positive_class = 2 * (t[:] - 4) > X_swiss[:, 1]\n",
    "X_pos = X_swiss[positive_class]\n",
    "X_neg = X_swiss[~positive_class]\n",
    "\n",
    "fig = plt.figure(figsize=(6, 5))\n",
    "ax = plt.subplot(1, 1, 1, projection='3d')\n",
    "ax.view_init(10, -70)\n",
    "ax.plot(X_neg[:, 0], X_neg[:, 1], X_neg[:, 2], \"y^\")\n",
    "ax.plot(X_pos[:, 0], X_pos[:, 1], X_pos[:, 2], \"gs\")\n",
    "ax.xaxis.set_rotate_label(False)\n",
    "ax.yaxis.set_rotate_label(False)\n",
    "ax.zaxis.set_rotate_label(False)\n",
    "ax.set_xlabel(\"$x_1$\", rotation=0)\n",
    "ax.set_ylabel(\"$x_2$\", rotation=0)\n",
    "ax.set_zlabel(\"$x_3$\", rotation=0)\n",
    "ax.set_xlim(axes[0:2])\n",
    "ax.set_ylim(axes[2:4])\n",
    "ax.set_zlim(axes[4:6])\n",
    "save_fig(\"manifold_decision_boundary_plot3\")\n",
    "plt.show()\n",
    "\n",
    "fig = plt.figure(figsize=(5, 4))\n",
    "ax = plt.subplot(1, 1, 1)\n",
    "ax.plot(t[positive_class], X_swiss[positive_class, 1], \"gs\")\n",
    "ax.plot(t[~positive_class], X_swiss[~positive_class, 1], \"y^\")\n",
    "ax.plot([4, 15], [0, 22], \"b-\", linewidth=2)\n",
    "ax.axis([4, 15, axes[2], axes[3]])\n",
    "ax.set_xlabel(\"$z_1$\")\n",
    "ax.set_ylabel(\"$z_2$\", rotation=0, labelpad=8)\n",
    "ax.grid(True)\n",
    "save_fig(\"manifold_decision_boundary_plot4\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47029639",
   "metadata": {},
   "source": [
    "## PCA\n",
    "\n",
    "First it identifies the hyperplane that lies closes to the data and then projects the data onto it.\n",
    "\n",
    "The right hyperplane is chosen based on the plane that preserves the maximum amount of variance, as it will most liekly lose less information than other projections. Mathematically, we want to choose the axis that minimizes the mean squared distance between the original dataset and its projection onto that axis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2b3868a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extra code – this cell generates and saves Figure 8–7\n",
    "\n",
    "angle = np.pi / 5\n",
    "stretch = 5\n",
    "m = 200\n",
    "\n",
    "np.random.seed(3)\n",
    "X_line = np.random.randn(m, 2) / 10\n",
    "X_line = X_line @ np.array([[stretch, 0], [0, 1]])  # stretch\n",
    "X_line = X_line @ [[np.cos(angle), np.sin(angle)],\n",
    "                   [np.sin(angle), np.cos(angle)]]  # rotate\n",
    "\n",
    "u1 = np.array([np.cos(angle), np.sin(angle)])\n",
    "u2 = np.array([np.cos(angle - 2 * np.pi / 6), np.sin(angle - 2 * np.pi / 6)])\n",
    "u3 = np.array([np.cos(angle - np.pi / 2), np.sin(angle - np.pi / 2)])\n",
    "\n",
    "X_proj1 = X_line @ u1.reshape(-1, 1)\n",
    "X_proj2 = X_line @ u2.reshape(-1, 1)\n",
    "X_proj3 = X_line @ u3.reshape(-1, 1)\n",
    "\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.subplot2grid((3, 2), (0, 0), rowspan=3)\n",
    "plt.plot([-1.4, 1.4], [-1.4 * u1[1] / u1[0], 1.4 * u1[1] / u1[0]], \"k-\",\n",
    "         linewidth=2)\n",
    "plt.plot([-1.4, 1.4], [-1.4 * u2[1] / u2[0], 1.4 * u2[1] / u2[0]], \"k--\",\n",
    "         linewidth=2)\n",
    "plt.plot([-1.4, 1.4], [-1.4 * u3[1] / u3[0], 1.4 * u3[1] / u3[0]], \"k:\",\n",
    "         linewidth=2)\n",
    "plt.plot(X_line[:, 0], X_line[:, 1], \"ro\", alpha=0.5)\n",
    "plt.arrow(0, 0, u1[0], u1[1], head_width=0.1, linewidth=4, alpha=0.9,\n",
    "          length_includes_head=True, head_length=0.1, fc=\"b\", ec=\"b\", zorder=10)\n",
    "plt.arrow(0, 0, u3[0], u3[1], head_width=0.1, linewidth=1, alpha=0.9,\n",
    "          length_includes_head=True, head_length=0.1, fc=\"b\", ec=\"b\", zorder=10)\n",
    "plt.text(u1[0] + 0.1, u1[1] - 0.05, r\"$\\mathbf{c_1}$\", color=\"blue\")\n",
    "plt.text(u3[0] + 0.1, u3[1], r\"$\\mathbf{c_2}$\", color=\"blue\")\n",
    "plt.xlabel(\"$x_1$\")\n",
    "plt.ylabel(\"$x_2$\", rotation=0)\n",
    "plt.axis([-1.4, 1.4, -1.4, 1.4])\n",
    "plt.grid()\n",
    "\n",
    "plt.subplot2grid((3, 2), (0, 1))\n",
    "plt.plot([-2, 2], [0, 0], \"k-\", linewidth=2)\n",
    "plt.plot(X_proj1[:, 0], np.zeros(m), \"ro\", alpha=0.3)\n",
    "plt.gca().get_yaxis().set_ticks([])\n",
    "plt.gca().get_xaxis().set_ticklabels([])\n",
    "plt.axis([-2, 2, -1, 1])\n",
    "plt.grid()\n",
    "\n",
    "plt.subplot2grid((3, 2), (1, 1))\n",
    "plt.plot([-2, 2], [0, 0], \"k--\", linewidth=2)\n",
    "plt.plot(X_proj2[:, 0], np.zeros(m), \"ro\", alpha=0.3)\n",
    "plt.gca().get_yaxis().set_ticks([])\n",
    "plt.gca().get_xaxis().set_ticklabels([])\n",
    "plt.axis([-2, 2, -1, 1])\n",
    "plt.grid()\n",
    "\n",
    "plt.subplot2grid((3, 2), (2, 1))\n",
    "plt.plot([-2, 2], [0, 0], \"k:\", linewidth=2)\n",
    "plt.plot(X_proj3[:, 0], np.zeros(m), \"ro\", alpha=0.3)\n",
    "plt.gca().get_yaxis().set_ticks([])\n",
    "plt.axis([-2, 2, -1, 1])\n",
    "plt.xlabel(\"$z_1$\")\n",
    "plt.grid()\n",
    "\n",
    "save_fig(\"pca_best_projection_plot\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "275c125a",
   "metadata": {},
   "source": [
    "\n",
    "One method to find the principal component of a training set is to do a standard matrix factorization technique called *singular value decomposition* (SVD) that can decompose the training set matrix X into the matri multiplication of 3 matrices. U Σ and V(transpose), where V contains the unit vectors that define all the PC that we are looking for."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99607886",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# X = [...]  # the small 3D dataset was created earlier in this notebook\n",
    "# Note that PCA assumes you have centered the data \n",
    "# so don't forget to center your data before using PCA yourself\n",
    "X_centered = X - X.mean(axis=0)\n",
    "U, s, Vt = np.linalg.svd(X_centered)\n",
    "c1 = Vt[0]\n",
    "c2 = Vt[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6fb83b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# first PC\n",
    "c1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d3a9e95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# second PC\n",
    "c2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7163da2f",
   "metadata": {},
   "source": [
    "Note: in principle, the SVD factorization algorithm returns three matrices, **U**, **Σ** and **V**, such that **X** = **UΣV**<sup>⊺</sup>, where **U** is an _m_ × _m_ matrix, **Σ** is an _m_ × _n_ matrix, and **V** is an _n_ × _n_ matrix. But the `svd()` function returns **U**, **s** and **V**<sup>⊺</sup> instead. **s** is the vector containing all the values on the main diagonal of the top _n_ rows of **Σ**. Since **Σ** is full of zeros elsewhere, your can easily reconstruct it from **s**, like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f49468c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extra code – shows how to construct Σ from s\n",
    "m, n = X.shape\n",
    "Σ = np.zeros_like(X_centered)\n",
    "Σ[:n, :n] = np.diag(s)\n",
    "assert np.allclose(X_centered, U @ Σ @ Vt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd7a54d2",
   "metadata": {},
   "source": [
    "Projection down to d dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60634fdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "Vt.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1af0788e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_centered.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bc7c90c",
   "metadata": {},
   "outputs": [],
   "source": [
    "W2 = Vt[:2].T # 3 x 2\n",
    "X2D = X_centered @ W2 # (60 x 3) x (3 x 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3f7b49b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf70d391",
   "metadata": {},
   "outputs": [],
   "source": [
    "# under scikit-learn\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "X2D = pca.fit_transform(X)\n",
    "X2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9316c90c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to find the explained variance ratio of each principal component\n",
    "pca.explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b925ad4",
   "metadata": {},
   "source": [
    "### Choosing the right number of dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "191b4cd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# performs PCA for the MNIST dataset to compute the minimum\n",
    "# number of dimensions required to preserve 95% of training set\n",
    "from sklearn.datasets import fetch_openml\n",
    "\n",
    "mnist = fetch_openml('mnist_784', as_frame=False)\n",
    "X_train, y_train = mnist.data[:60000], mnist.target[:60000]\n",
    "X_test, y_test = mnist.data[60000:], mnist.target[60000:]\n",
    "\n",
    "pca = PCA()\n",
    "pca.fit(X_train)\n",
    "cumsum = np.cumsum(pca.explained_variance_ratio_)\n",
    "\n",
    "d = np.argmax(cumsum >= 0.95) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "885a8b87",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=d)\n",
    "X_reduced = pca.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "270743f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca.n_components_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2e89b81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extra code – this cell generates and saves Figure 8–8\n",
    "\n",
    "plt.figure(figsize=(6, 4))\n",
    "plt.plot(cumsum, linewidth=3)\n",
    "plt.axis([0, 400, 0, 1])\n",
    "plt.xlabel(\"Dimensions\")\n",
    "plt.ylabel(\"Explained Variance\")\n",
    "plt.plot([d, d], [0, 0.95], \"k:\")\n",
    "plt.plot([0, d], [0.95, 0.95], \"k:\")\n",
    "plt.plot(d, 0.95, \"ko\")\n",
    "plt.annotate(\"Elbow\", xy=(65, 0.85), xytext=(70, 0.7),\n",
    "             arrowprops=dict(arrowstyle=\"->\"))\n",
    "plt.grid(True)\n",
    "save_fig(\"explained_variance_plot\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be7e246e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using PCA as part of preprocessing step for\n",
    "# supervised learning task\n",
    "# the follwoing code create a 2-step pipeline\n",
    "# 1. Reduce dimensionality using PCA\n",
    "# 2. Classify using random forest\n",
    "# 3. Use randomizedseach CV to find a good combination of \n",
    "# hyperparameter for both PCA and the RF classifier\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "clf = make_pipeline(\n",
    "    PCA(random_state=42),\n",
    "    RandomForestClassifier(random_state=42)\n",
    ")\n",
    "param_distrib = {\n",
    "    \"pca__n_components\": np.arange(10,80),\n",
    "    \"randomforestclassifier__n_estimators\": np.arange(50,500)\n",
    "}\n",
    "\n",
    "rnd_search = RandomizedSearchCV(clf, param_distrib,\n",
    "                                n_iter=10, cv=3,\n",
    "                                random_state=42)\n",
    "rnd_search.fit(X_train[:1000], y_train[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd4dfe8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(rnd_search.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9215f1c",
   "metadata": {},
   "source": [
    "Note that if we have used a linear model instead, the search would have preserved more dimensions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5de3455f",
   "metadata": {},
   "source": [
    "### PCA for Compression\n",
    "\n",
    "We can use the inverse_transform to project back the PCA to the original data, but as the projection lost a bit of information, while we cannot recover this loss, it will still likely be close to the oiginal data. The mean squared distance between the original data and reconstructed data is called the *reconstruction error*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc5dc552",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_recovered = pca.inverse_transform(X_reduced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b11f4c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extra code – this cell generates and saves Figure 8–9\n",
    "\n",
    "plt.figure(figsize=(7, 4))\n",
    "for idx, X in enumerate((X_train[::2100], X_recovered[::2100])):\n",
    "    plt.subplot(1, 2, idx + 1)\n",
    "    plt.title([\"Original\", \"Compressed\"][idx])\n",
    "    for row in range(5):\n",
    "        for col in range(5):\n",
    "            plt.imshow(X[row * 5 + col].reshape(28, 28), cmap=\"binary\",\n",
    "                       vmin=0, vmax=255, extent=(row, row + 1, col, col + 1))\n",
    "            plt.axis([0, 5, 0, 5])\n",
    "            plt.axis(\"off\")\n",
    "\n",
    "save_fig(\"mnist_compression_plot\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70fa17e1",
   "metadata": {},
   "source": [
    "### Randomized PCA\n",
    "\n",
    "Setting the svd_solver to \"randomized\" will result in sklearn to use a stocahstic algorihtm called *randomized* PCA that quickly finds an approximation of the first d principal components. This results in a computation compleixty of O(m x d^2) + O(d^3), instead of the O(m x n^2) + O(n^3) in the original SVD approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9260609",
   "metadata": {},
   "outputs": [],
   "source": [
    "rnd_pca = PCA(n_components=154, svd_solver=\"randomized\", random_state=42)\n",
    "X_reduced = rnd_pca.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5da06e07",
   "metadata": {},
   "source": [
    "# Incremental PCA\n",
    "\n",
    "This algorithm allows us to split the training set into mini-batches and feed these in one mini-batch at a time. Useful for large training sets and for applying PCA online."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6f754e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import IncrementalPCA\n",
    "\n",
    "n_batches = 100\n",
    "inc_pca = IncrementalPCA(n_components=154)\n",
    "\n",
    "for X_batch in np.array_split(X_train, n_batches):\n",
    "    inc_pca.partial_fit(X_batch)\n",
    "\n",
    "X_reduced = inc_pca.transform(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eac231ec",
   "metadata": {},
   "source": [
    "Using Numpy's memmap class to manipulate a large array stored in a binary file on disk as if it were entirely in memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "132d5421",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"my_mnist.mmap\"\n",
    "X_mmap = np.memmap(filename, dtype='float32', mode='write', shape=X_train.shape)\n",
    "X_mmap[:] = X_train  # could be a loop instead, saving the data chunk by chunk\n",
    "X_mmap.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba4101e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_mmap = np.memmap(filename, dtype=\"float32\", mode=\"readonly\").reshape(-1, 784)\n",
    "batch_size = X_mmap.shape[0] // n_batches\n",
    "inc_pca = IncrementalPCA(n_components=154, batch_size=batch_size)\n",
    "inc_pca.fit(X_mmap)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36932bb5",
   "metadata": {},
   "source": [
    "# Random Projection\n",
    "\n",
    "Consider using random projection when you have many features (tens of thousands like an image)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b3fae25",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.random_projection import johnson_lindenstrauss_min_dim\n",
    "\n",
    "m , e = 5000, 0.1\n",
    "d = johnson_lindenstrauss_min_dim(m, eps = e)\n",
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27435182",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now generate a random Matrix P of shape [d,n]\n",
    "\n",
    "n = 20000\n",
    "np.random.seed(42)\n",
    "P = np.random.randn(d, n) / np.sqrt(d) # std dev\n",
    "\n",
    "X = np.random.randn(m,n)\n",
    "X_reduced = X @ P.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74ba8d05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sklearn offers the GaussianRandomProjection class\n",
    "from sklearn.random_projection import GaussianRandomProjection\n",
    "\n",
    "gaussian_rnd_proj = GaussianRandomProjection(eps = e, random_state=42)\n",
    "X_reduced = gaussian_rnd_proj.fit_transform(X) # same result as above"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6f5b403",
   "metadata": {},
   "source": [
    "Note that sklearn also has SparseRandomProjection to generate a random matrix of the same shape, but the random matrix is spares (meaning it uses less memory and is much faster). Highly preferably to use this rather than the GaussianRandomProjection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c3006a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to compute the inverse transform\n",
    "\n",
    "components_pinv = np.linalg.pinv(gaussian_rnd_proj.components_)\n",
    "X_recovered = X_reduced @ components_pinv.T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b266b975",
   "metadata": {},
   "source": [
    "Random Projection -> Simple, fast, memory efficient and powerful dimensionality reduction algorihtm. Can be especially useful when dealing with high-dimensional datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9571bc3",
   "metadata": {},
   "source": [
    "For further reference look up *locality sensitive hashing* (LSH). Also random projection are not always used to reduce the dimensionality of large datasets, as it also happend in thebrain of a fruit fly that implemens an analog of random projection to map dense low-dimensional olfactory inputs to sparse high-dimensional binary outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cef3fed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Gaussian random projection on the MNIST data\n",
    "# # was attempted but unable to be done because \n",
    "# # the features are too small\n",
    "# # compared to the number of samples\n",
    "# X_reduced = gaussian_rnd_proj.fit_transform(X_train)\n",
    "# components_pinv = np.linalg.pinv(gaussian_rnd_proj.components_)\n",
    "# X_recovered = X_reduced @ components_pinv.T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63788e22",
   "metadata": {},
   "source": [
    "## LLE / Locally Linear Embedding\n",
    "\n",
    "A non linear dimensionality reduction (NLDR) that acts as a manifold learning technique. It works by first measure how each trianing instace linearly relates to its nearest neighbors and then looks for a low-dimensional representation of the training set where these local relationships are best preserved.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4168728b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_swiss_roll\n",
    "from sklearn.manifold import LocallyLinearEmbedding\n",
    "\n",
    "# same swiss roll as the one above\n",
    "X_swiss, t = make_swiss_roll(n_samples = 1000, noise = 0.2, random_state=42)\n",
    "lle = LocallyLinearEmbedding(n_components=2, n_neighbors=10, random_state=42)\n",
    "X_unrolled = lle.fit_transform(X_swiss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08d24f68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extra code – this cell generates and saves Figure 8–10\n",
    "\n",
    "plt.scatter(X_unrolled[:, 0], X_unrolled[:, 1],\n",
    "            c=t, cmap=darker_hot)\n",
    "plt.xlabel(\"$z_1$\")\n",
    "plt.ylabel(\"$z_2$\", rotation=0)\n",
    "plt.axis([-0.055, 0.060, -0.070, 0.090])\n",
    "plt.grid(True)\n",
    "\n",
    "save_fig(\"lle_unrolling_plot\")\n",
    "plt.title(\"Unrolled swiss roll using LLE\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e29ea9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extra code – shows how well correlated z1 is to t: LLE worked fine\n",
    "plt.title(\"$z_1$ vs $t$\")\n",
    "plt.scatter(X_unrolled[:, 0], t, c=t, cmap=darker_hot)\n",
    "plt.xlabel(\"$z_1$\")\n",
    "plt.ylabel(\"$t$\", rotation=0)\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2edb5d70",
   "metadata": {},
   "source": [
    "Computational Complexity:\n",
    "- O($m log(m)n log(k)$) for finding the k-nearest neighbors\n",
    "- O($mnk^3$) for optimizing the weights\n",
    "- O($dm^2$) for constructing the low-dimensional representations\n",
    "\n",
    "Due to the $m^2$ terms (number of instances / dataset), the algorithm does not scale well for very large datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "784bd5b1",
   "metadata": {},
   "source": [
    "# Other Dimensionality Reduction Techniques\n",
    "- sklearn.manifold.MDS (Multidimensional scaling) <br>\n",
    "Reduces dimensionality while trying to preserve the distances between the instances.\n",
    "\n",
    "- sklearn.manifold.Isomap <br>\n",
    "Creates a graph connecting each instance to its nearest neighbors, then reduces dimensionality while trying to preserve the *geodesic distances* (number of nodes on the shortest path between these nodes).\n",
    "\n",
    "- sklearn.manifold.TSNE (t-distributed stochastic neighbor embedding) <br>\n",
    "Reduces dimensionality while trying to keep similar instances close and dissimilar instances apart. Mostly used for visualization, particularly to visualize clusters of instance in high-dimensional space.\n",
    "\n",
    "- klearn.disciminant_analysis.LinearDiscriminantAnalysis <br>\n",
    "Linear classification algorithm that during training, learns the most discriminative axes between the classes. These axes can then  be used to define a hyperplane onto which to project the data with the benefit where the projection will keep classes as far apart as possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92590979",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import MDS\n",
    "\n",
    "mds = MDS(n_components=2, normalized_stress=False, random_state=42)\n",
    "X_reduced_mds = mds.fit_transform(X_swiss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a53e2e58",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import Isomap\n",
    "\n",
    "isomap = Isomap(n_components=2)\n",
    "X_reduced_isomap = isomap.fit_transform(X_swiss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10c2a083",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "\n",
    "tsne = TSNE(n_components=2, init=\"random\", learning_rate=\"auto\",\n",
    "            random_state=42)\n",
    "X_reduced_tsne = tsne.fit_transform(X_swiss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ab945c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extra code – this cell generates and saves Figure 8–11\n",
    "\n",
    "titles = [\"MDS\", \"Isomap\", \"t-SNE\"]\n",
    "\n",
    "plt.figure(figsize=(11, 4))\n",
    "\n",
    "for subplot, title, X_reduced in zip((131, 132, 133), titles,\n",
    "                                     (X_reduced_mds, X_reduced_isomap, X_reduced_tsne)):\n",
    "    plt.subplot(subplot)\n",
    "    plt.title(title)\n",
    "    plt.scatter(X_reduced[:, 0], X_reduced[:, 1], c=t, cmap=darker_hot)\n",
    "    plt.xlabel(\"$z_1$\")\n",
    "    if subplot == 131:\n",
    "        plt.ylabel(\"$z_2$\", rotation=0)\n",
    "    plt.grid(True)\n",
    "\n",
    "save_fig(\"other_dim_reduction_plot\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daf7edae",
   "metadata": {},
   "source": [
    "- MDS was able to flatten the Swiss roll without losing its global curvature\n",
    "- Isomap drops the curvature\n",
    "- t-SNE flattens the Swiss roll and preserves a bit of the curvaturet and amplifies the clusters (which may or may not be a good thing)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf586dd0",
   "metadata": {},
   "source": [
    "# Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "757b0d16",
   "metadata": {},
   "source": [
    "# 1. What are the main motivations for reducing a dataset’s dimensionality? What are the main drawbacks?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4513dca8",
   "metadata": {},
   "source": [
    "Motivation\n",
    "- Issues of linear separation cannot be done in higher dimension but could possibly be done in lower dimension\n",
    "- Optimizing the time for training when the original dataset involves thousands / millions of features.\n",
    "- Data visualization to identify clusters / patterns in the data\n",
    "- To save space\n",
    "\n",
    "\n",
    "Drawback:\n",
    "- Some information is lost\n",
    "- Can be computationally intensive\n",
    "- Loses the interpretability of the dataset\n",
    "- Assumes certain structure to the dataset depending on the dimensionality reduction technique that was chosen."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bc24499",
   "metadata": {},
   "source": [
    "# 2. What is the curse of dimensionality?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11070cfa",
   "metadata": {},
   "source": [
    "The higher the number of features, the further away points in higher dimensions are within each other and the harder to accurately predict new instances in higher dimensions (aka risk of overfitting). While this can be overcomed with having more data, unfortunately the scaling of the number of data needed grows exponentially with the number of dimensions and it is most often impossible to have the minimum required number of instances for break the curse of dimensionality."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a05009b7",
   "metadata": {},
   "source": [
    "# 3. Once a dataset's dimensionality has been reduced, is it possible to reverse the operation? If so, how? if not, why?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ae8b990",
   "metadata": {},
   "source": [
    "For PCA while it is possible to recover the original data set (by multiplying the transformed dataset with the inverse of the loadings), there will be some loss in information in the recovered dataset.\n",
    "\n",
    "For other techniques such as t-sne, then the process cannot be undone."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "852df33b",
   "metadata": {},
   "source": [
    "# 4. Can PCA be used to reduce the dimensionality of a highly nonlinear dataset? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ca163d5",
   "metadata": {},
   "source": [
    "PCA can be used to reduce dimensionality, even for highly nonlinear ones, as it can get rid of useless dimensions. But if there are no useless dimension, then reducing dimensionality with PCA will lose too much information.\n",
    "\n",
    "PCA finds a hyperplane that can explain the variance of the dataset the most, which might not necessarily be the correct method for dimensionality reduction (remember the Swiss roll data set, if PCA is used, then it will be projected in a plane, and not \"unfolded\" when compared to other manifold projection method)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65feaef0",
   "metadata": {},
   "source": [
    "# 5. Suppose you perform PCA on a 1,000 dimensional dataset, setting the explained variance ratio to 95%. How many dimensions will the resulting dataset have?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ce41be6",
   "metadata": {},
   "source": [
    "Depends on the nature of the dataset, it can be any number between 1 to 950."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1fbde2b",
   "metadata": {},
   "source": [
    "# 6. In what cases would you use regular PCA, incremental PCA, randomized PCA or random projection?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d6652d6",
   "metadata": {},
   "source": [
    "- Regular PCA: for lower dimension and small datasets\n",
    "- Incremental PCA: for larger datasets that don't fit in memory (but slower than regular PCA). Also useful for online PCA when new data arrives regularly.\n",
    "- Randomized PCA: when there are too many features and the data fits the memory (much faster than regular PCA)\n",
    "- Random projection: For very high dimensional datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b46ee2b",
   "metadata": {},
   "source": [
    "# 7. How can you evaluate the performance of a dimensionality reduction algorithm on your dataset?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e8a9a3d",
   "metadata": {},
   "source": [
    "One way is to apply the reverse transformation and measure the reconstruction error (for algorithms that have reverse transformations).\n",
    "\n",
    "\n",
    "By having the dimensionality reduction be a preprocessing step before using a certain machine learning classification / regression algorithm, finding out the CV validation error rate and then compare it across different dimensionality reduction algorithms. If the dimensionality reduction algorithm did not lose too much information, then the machine learning algorithm should perform just as well as when using the original dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d72919e9",
   "metadata": {},
   "source": [
    "# 8. Does it make any sense to chain two different dimensionality reduction algorithms?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7baeba0f",
   "metadata": {},
   "source": [
    "It can absolutely make sense to chain two different dimensionality reduction algorithms. A common example is using PCA or Random Projection to quickly get rid of a large number of useless dimensions, then applying another much slower dimensionality reduction algorithm, such as LLE. This two-step approach will likely yield roughly the same performance as using LLE only, but in a fraction of the time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0942caa",
   "metadata": {},
   "source": [
    "# 9. Load the MNIST dataset (introduced in Chapter 3) and split it into a training set and a test set (take the first 60,000 instances for training, and the remaining 10,000 for testing). \n",
    "- Train a random forest classifier on the dataset and time how long it takes, then evaluate the resulting model on the test set. \n",
    "- Next, use PCA to reduce the dataset’s dimensionality, with an explained variance ratio of 95%. \n",
    "- Train a new random forest classifier on the reduced dataset and see how long it takes. \n",
    "- Was training much faster? \n",
    "- Next, evaluate the classifier on the test set. How does it compare to the previous classifier? Try again with an SGDClassifier. How much does PCA help now?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d860e2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = mnist.data[:60000]\n",
    "y_train = mnist.target[:60000]\n",
    "\n",
    "X_test = mnist.data[60000:]\n",
    "y_test = mnist.target[60000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d30067a",
   "metadata": {},
   "outputs": [],
   "source": [
    "rnd_clf = RandomForestClassifier(n_estimators=100, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c575ba58",
   "metadata": {},
   "outputs": [],
   "source": [
    "%time rnd_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9e52411",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "y_pred = rnd_clf.predict(X_test)\n",
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b08927d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use PCA to reduce the dataset's dimensionality\n",
    "# with 95% explained variance ratio\n",
    "pca = PCA(n_components=0.95)\n",
    "X_reduced = pca.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d526347",
   "metadata": {},
   "outputs": [],
   "source": [
    "%time rnd_clf.fit(X_reduced, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e5dbc53",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_reduced = pca.transform(X_test)\n",
    "y_pred = rnd_clf.predict(X_test_reduced)\n",
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ca05626",
   "metadata": {},
   "source": [
    "Is worse and runs longer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bfca5a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "clf = SGDClassifier(random_state= 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c154c86",
   "metadata": {},
   "outputs": [],
   "source": [
    "%time clf.fit(X_reduced, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb519dec",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = clf.predict(X_test_reduced)\n",
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56658088",
   "metadata": {},
   "source": [
    "PCA can give a speedup, but not always! And if lucky, then a performance boost too!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a4adda5",
   "metadata": {},
   "source": [
    "# 10. Use t-SNE to reduce the first 5,000 images of the MNIST dataset down to 2 dimensions and plot the result using Matplotlib.\n",
    "You can use a scatterplot using 10 different colors to represent each image’s target class. Alternatively, you can replace each dot in the scatterplot with the corresponding instance’s class (a digit from 0 to 9), or even plot scaled-down versions of the digit images themselves (if you plot all digits the visualization will be too cluttered, so you should either draw a random sample or plot an instance only if no other instance has already been plotted at a close distance). You should get a nice visualization with well- separated clusters of digits. Try using other dimensionality reduction algorithms, such as PCA, LLE, or MDS, and compare the resulting visualizations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba28a813",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "X = mnist.data[:5000]\n",
    "y = mnist.target[:5000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3722ba7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "\n",
    "tsne = TSNE(n_components=2, init=\"random\", learning_rate=\"auto\",\n",
    "            random_state=42)\n",
    "X_reduced_tsne = tsne.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08b59fcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.colors as mcolors\n",
    "\n",
    "color_map = {\n",
    "    '0': 'aqua',\n",
    "    '1': 'azure',\n",
    "    '2': 'black',\n",
    "    '3': 'brown',\n",
    "    '4': 'purple',\n",
    "    '5': 'red',\n",
    "    '6': 'lavender',\n",
    "    '7': 'lightblue',\n",
    "    '8': 'lime',\n",
    "    '9': 'pink'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15429e5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_df = pd.DataFrame(y)\n",
    "y_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24da3191",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_color_df = y_df.replace(color_map)\n",
    "y_color_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c9301f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.colors as mcolors\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.title(\"T-SNE of MNIST Data\")\n",
    "plt.scatter(X_reduced_tsne[:, 0], X_reduced_tsne[:, 1], c = y.astype(int), alpha= 0.4, cmap = \"jet\")\n",
    "plt.ylabel(\"$z_2$\", rotation=0)\n",
    "plt.colorbar()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b310a495",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(9, 9))\n",
    "cmap = plt.cm.jet\n",
    "for digit in ('4', '9'):\n",
    "    plt.scatter(X_reduced_tsne[y == digit, 0], X_reduced_tsne[y == digit, 1],\n",
    "                c=[cmap(float(digit) / 9)], alpha=0.5)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc61d93d",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = (y == '4') | (y == '9')\n",
    "X_subset = X[idx]\n",
    "y_subset = y[idx]\n",
    "\n",
    "tsne_subset = TSNE(n_components=2, init=\"random\", learning_rate=\"auto\",\n",
    "                   random_state=42)\n",
    "X_subset_reduced = tsne_subset.fit_transform(X_subset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "460330e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(9, 9))\n",
    "for digit in ('4', '9'):\n",
    "    plt.scatter(X_subset_reduced[y_subset == digit, 0],\n",
    "                X_subset_reduced[y_subset == digit, 1],\n",
    "                c=[cmap(float(digit) / 9)], alpha=0.5)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74bfa75c",
   "metadata": {},
   "source": [
    "Let's create a `plot_digits()` function that will draw a scatterplot (similar to the above scatterplots) plus write colored digits, with a minimum distance guaranteed between these digits. If the digit images are provided, they are plotted instead. This implementation was inspired from one of Scikit-Learn's excellent examples ([plot_lle_digits](https://scikit-learn.org/stable/auto_examples/manifold/plot_lle_digits.html), based on a different digit dataset)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "219e469d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from matplotlib.offsetbox import AnnotationBbox, OffsetImage\n",
    "\n",
    "def plot_digits(X, y, min_distance=0.04, images=None, figsize=(13, 10)):\n",
    "    # Let's scale the input features so that they range from 0 to 1\n",
    "    X_normalized = MinMaxScaler().fit_transform(X)\n",
    "    # Now we create the list of coordinates of the digits plotted so far.\n",
    "    # We pretend that one is already plotted far away at the start, to\n",
    "    # avoid `if` statements in the loop below\n",
    "    neighbors = np.array([[5., 5.]])\n",
    "    # The rest should be self-explanatory\n",
    "    plt.figure(figsize=figsize)\n",
    "    cmap = plt.cm.jet\n",
    "    digits = np.unique(y)\n",
    "    for digit in digits:\n",
    "        plt.scatter(X_normalized[y == digit, 0], X_normalized[y == digit, 1],\n",
    "                    c=[cmap(float(digit) / 9)], alpha=0.5)\n",
    "    plt.axis(\"off\")\n",
    "    ax = plt.gca()  # get current axes\n",
    "    for index, image_coord in enumerate(X_normalized):\n",
    "        closest_distance = np.linalg.norm(neighbors - image_coord, axis=1).min()\n",
    "        if closest_distance > min_distance:\n",
    "            neighbors = np.r_[neighbors, [image_coord]]\n",
    "            if images is None:\n",
    "                plt.text(image_coord[0], image_coord[1], str(int(y[index])),\n",
    "                         color=cmap(float(y[index]) / 9),\n",
    "                         fontdict={\"weight\": \"bold\", \"size\": 16})\n",
    "            else:\n",
    "                image = images[index].reshape(28, 28)\n",
    "                imagebox = AnnotationBbox(OffsetImage(image, cmap=\"binary\"),\n",
    "                                          image_coord)\n",
    "                ax.add_artist(imagebox)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8dcb242",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_digits(X_reduced_tsne, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "610cbd5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_digits(X_reduced_tsne, y, images=X, figsize=(35, 25))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df8cd313",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the digits for 4s and 9s\n",
    "plot_digits(X_subset_reduced, y_subset, images=X_subset, figsize=(22, 22))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8852f058",
   "metadata": {},
   "source": [
    "Notice how similar-looking 4s are grouped together. For example, the 4s get more and more inclined as they approach the top of the figure. The inclined 9s are also closer to the top. Some 4s really do look like 9s, and vice versa."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00199b86",
   "metadata": {},
   "source": [
    "When PCA, LLE and MDS is used (even when used one after another) it did not result into clusters as nice as t-SNE."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
