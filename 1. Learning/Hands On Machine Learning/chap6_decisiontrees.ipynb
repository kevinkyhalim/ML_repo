{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2501cf93",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.rc('font', size=14)\n",
    "plt.rc('axes', labelsize=14, titlesize=14)\n",
    "plt.rc('legend', fontsize=14)\n",
    "plt.rc('xtick', labelsize=10)\n",
    "plt.rc('ytick', labelsize=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "395dc0ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "IMAGES_PATH = Path() / \"images\" / \"decision_trees\"\n",
    "IMAGES_PATH.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def save_fig(fig_id, tight_layout=True, fig_extension=\"png\", resolution=300):\n",
    "    path = IMAGES_PATH / f\"{fig_id}.{fig_extension}\"\n",
    "    if tight_layout:\n",
    "        plt.tight_layout()\n",
    "    plt.savefig(path, format=fig_extension, dpi=resolution)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99e55ae2",
   "metadata": {},
   "source": [
    "# Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b61b96b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "iris = load_iris(as_frame=True)\n",
    "X_iris = iris.data[[\"petal length (cm)\", \"petal width (cm)\"]].values\n",
    "y_iris = iris.target\n",
    "\n",
    "tree_clf = DecisionTreeClassifier(max_depth=2, random_state=42)\n",
    "tree_clf.fit(X_iris, y_iris)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a5aba09",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import export_graphviz\n",
    "\n",
    "export_graphviz(\n",
    "    tree_clf,\n",
    "    out_file=str(IMAGES_PATH / \"iris_tree.dot\"),\n",
    "    feature_names=[\"petal length (cm)\", \"petal width (cm)\"],\n",
    "    class_names=iris.target_names,\n",
    "    rounded=True,\n",
    "    filled=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b293b1d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from graphviz import Source\n",
    "\n",
    "Source.from_file(IMAGES_PATH / \"iris_tree.dot\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "703637ef",
   "metadata": {},
   "source": [
    "Gini measures the *Gini impurity* where a node is \"pure\" (gini = 0) if all the training instances it applies to belong to the same class!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2fa553d",
   "metadata": {},
   "source": [
    "Note that while scikit-learn uses the CART (Classification and Regression Tree) algorithm that produces only *binary tree*, there are other algorihtms (e.g. ID3) that can produce trees with nodes that have MORE than 2 children."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "779c77ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "help(tree_clf.tree_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a78f9964",
   "metadata": {},
   "source": [
    "# Making Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adf25a39",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# extra code – just formatting details\n",
    "from matplotlib.colors import ListedColormap\n",
    "custom_cmap = ListedColormap(['#fafab0', '#9898ff', '#a0faa0'])\n",
    "plt.figure(figsize=(8, 4))\n",
    "\n",
    "lengths, widths = np.meshgrid(np.linspace(0, 7.2, 100), np.linspace(0, 3, 100))\n",
    "X_iris_all = np.c_[lengths.ravel(), widths.ravel()]\n",
    "y_pred = tree_clf.predict(X_iris_all).reshape(lengths.shape)\n",
    "plt.contourf(lengths, widths, y_pred, alpha=0.3, cmap=custom_cmap)\n",
    "for idx, (name, style) in enumerate(zip(iris.target_names, (\"yo\", \"bs\", \"g^\"))):\n",
    "    plt.plot(X_iris[:, 0][y_iris == idx], X_iris[:, 1][y_iris == idx],\n",
    "             style, label=f\"Iris {name}\")\n",
    "\n",
    "# extra code – this section beautifies and saves Figure 6–2\n",
    "tree_clf_deeper = DecisionTreeClassifier(max_depth=3, random_state=42)\n",
    "tree_clf_deeper.fit(X_iris, y_iris)\n",
    "th0, th1, th2a, th2b = tree_clf_deeper.tree_.threshold[[0, 2, 3, 6]]\n",
    "plt.xlabel(\"Petal length (cm)\")\n",
    "plt.ylabel(\"Petal width (cm)\")\n",
    "plt.plot([th0, th0], [0, 3], \"k-\", linewidth=2)\n",
    "plt.plot([th0, 7.2], [th1, th1], \"k--\", linewidth=2)\n",
    "plt.plot([th2a, th2a], [0, th1], \"k:\", linewidth=2)\n",
    "plt.plot([th2b, th2b], [th1, 3], \"k:\", linewidth=2)\n",
    "plt.text(th0 - 0.05, 1.0, \"Depth=0\", horizontalalignment=\"right\", fontsize=15)\n",
    "plt.text(3.2, th1 + 0.02, \"Depth=1\", verticalalignment=\"bottom\", fontsize=13)\n",
    "plt.text(th2a + 0.05, 0.5, \"(Depth=2)\", fontsize=11)\n",
    "plt.axis([0, 7.2, 0, 3])\n",
    "plt.legend()\n",
    "save_fig(\"decision_tree_decision_boundaries_plot\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4f3900e",
   "metadata": {},
   "source": [
    "# Estimating Class Probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5ae62db",
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_clf.predict_proba([[5, 1.5]]).round(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "647a7080",
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_clf.predict([[5, 1.5]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fa3e71b",
   "metadata": {},
   "source": [
    "# Computational Complexity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63a5e3ee",
   "metadata": {},
   "source": [
    "Traversing a decision tree requires going through roughly O($log_2$(m)) nodes, where $log_2$(m) is the *binary logarithm* of m, equal to log(m) / log(2). Since each nodes only requires the checking of value of one feature, overall prediction complexity is not affected by the number of features and hence stays O($log_2$(m)).\n",
    "\n",
    "\n",
    "Though for the training algorithm, since the algorithm will compare all features on all samples at each node, this will result in a training complexity of O(n x m $log_2$(m))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7d5fd25",
   "metadata": {},
   "source": [
    "By default, the DecisionTreeClassifier uses the Gini impurity measure, but the *entropy* impurity measure can also be used by setting the criterion hyperparameter to \"entropy\" (comes from thermodynamics where entropy approaches 0 when molecules are still and well ordered). In information theory, it measures the average information content of a message. When it is 0, all messages are identical.\n",
    "\n",
    "In machine learning, a set's entropy is - when it contains instances of only 1 class."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d97a1589",
   "metadata": {},
   "source": [
    "In general, it doesn't really matter whether we use Gini or Entropy, but note that:\n",
    "- Gini impurity is slightly faster to compute\n",
    "- When differing, Gini tends to isolate the most frequent class in its own branch of the tree, while entropy tends to produce slightly more balanced trees."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b97645c5",
   "metadata": {},
   "source": [
    "## Regularization Hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68601ced",
   "metadata": {},
   "source": [
    "To avoid overfitting, several hyperparameters should be set.\n",
    "- maximum depth of the tree (max_depth)\n",
    "- maximum number of features evaluated for splitting at each node (max_features)\n",
    "- maximum number of leaf nodes (max_leaf_nodes)\n",
    "- minimum number of samples a node must have before it can be split (min_samples_split)\n",
    "- minimum number of samples a node must have before it can be split, but expressed as a fraction of total number of weighted instances (min_weight_fraction_leaf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f41b6cc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_moons\n",
    "X_moons, y_moons = make_moons (n_samples = 150, noise = 0.2, random_state = 42)\n",
    "\n",
    "tree_clf1 = DecisionTreeClassifier(random_state=42)\n",
    "tree_clf2 = DecisionTreeClassifier(min_samples_leaf=5, random_state=42)\n",
    "tree_clf1.fit(X_moons, y_moons)\n",
    "tree_clf2.fit(X_moons, y_moons)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed26ba55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extra code – this cell generates and saves Figure 6–3\n",
    "\n",
    "def plot_decision_boundary(clf, X, y, axes, cmap):\n",
    "    x1, x2 = np.meshgrid(np.linspace(axes[0], axes[1], 100),\n",
    "                         np.linspace(axes[2], axes[3], 100))\n",
    "    X_new = np.c_[x1.ravel(), x2.ravel()]\n",
    "    y_pred = clf.predict(X_new).reshape(x1.shape)\n",
    "    \n",
    "    plt.contourf(x1, x2, y_pred, alpha=0.3, cmap=cmap)\n",
    "    plt.contour(x1, x2, y_pred, cmap=\"Greys\", alpha=0.8)\n",
    "    colors = {\"Wistia\": [\"#78785c\", \"#c47b27\"], \"Pastel1\": [\"red\", \"blue\"]}\n",
    "    markers = (\"o\", \"^\")\n",
    "    for idx in (0, 1):\n",
    "        plt.plot(X[:, 0][y == idx], X[:, 1][y == idx],\n",
    "                 color=colors[cmap][idx], marker=markers[idx], linestyle=\"none\")\n",
    "    plt.axis(axes)\n",
    "    plt.xlabel(r\"$x_1$\")\n",
    "    plt.ylabel(r\"$x_2$\", rotation=0)\n",
    "\n",
    "fig, axes = plt.subplots(ncols=2, figsize=(10, 4), sharey=True)\n",
    "plt.sca(axes[0])\n",
    "plot_decision_boundary(tree_clf1, X_moons, y_moons,\n",
    "                       axes=[-1.5, 2.4, -1, 1.5], cmap=\"Wistia\")\n",
    "plt.title(\"No restrictions\")\n",
    "plt.sca(axes[1])\n",
    "plot_decision_boundary(tree_clf2, X_moons, y_moons,\n",
    "                       axes=[-1.5, 2.4, -1, 1.5], cmap=\"Wistia\")\n",
    "plt.title(f\"min_samples_leaf = {tree_clf2.min_samples_leaf}\")\n",
    "plt.ylabel(\"\")\n",
    "save_fig(\"min_samples_leaf_plot\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4241d94",
   "metadata": {},
   "source": [
    "The figure on the right (regularized) will most likely generalize better, compared to the figure on the left which seems like it is overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "992f9cec",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_moons_test, y_moons_test = make_moons(n_samples=1000, noise=0.2,\n",
    "                                        random_state=43)\n",
    "tree_clf1.score(X_moons_test, y_moons_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "293fa125",
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_clf2.score(X_moons_test, y_moons_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bc0fee7",
   "metadata": {},
   "source": [
    "# Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b94f55c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "np.random.seed(42)\n",
    "X_quad = np.random.rand(200,1) - 0.5\n",
    "y_quad = X_quad ** 2 + 0.025 * np.random.randn(200,1)\n",
    "\n",
    "tree_reg = DecisionTreeRegressor(max_depth=2, random_state=42)\n",
    "tree_reg.fit(X_quad, y_quad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25fb913b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extra code – we've already seen how to use export_graphviz()\n",
    "export_graphviz(\n",
    "    tree_reg,\n",
    "    out_file=str(IMAGES_PATH / \"regression_tree.dot\"),\n",
    "    feature_names=[\"x1\"],\n",
    "    rounded=True,\n",
    "    filled=True\n",
    ")\n",
    "Source.from_file(IMAGES_PATH / \"regression_tree.dot\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9024eb55",
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_reg2 = DecisionTreeRegressor(max_depth=3, random_state=42)\n",
    "tree_reg2.fit(X_quad, y_quad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec1624d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_reg.tree_.threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b8f5559",
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_reg2.tree_.threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7b669a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extra code – this cell generates and saves Figure 6–5\n",
    "\n",
    "def plot_regression_predictions(tree_reg, X, y, axes=[-0.5, 0.5, -0.05, 0.25]):\n",
    "    x1 = np.linspace(axes[0], axes[1], 500).reshape(-1, 1)\n",
    "    y_pred = tree_reg.predict(x1)\n",
    "    plt.axis(axes)\n",
    "    plt.xlabel(\"$x_1$\")\n",
    "    plt.plot(X, y, \"b.\")\n",
    "    plt.plot(x1, y_pred, \"r.-\", linewidth=2, label=r\"$\\hat{y}$\")\n",
    "\n",
    "fig, axes = plt.subplots(ncols=2, figsize=(10, 4), sharey=True)\n",
    "plt.sca(axes[0])\n",
    "plot_regression_predictions(tree_reg, X_quad, y_quad)\n",
    "\n",
    "th0, th1a, th1b = tree_reg.tree_.threshold[[0, 1, 4]]\n",
    "for split, style in ((th0, \"k-\"), (th1a, \"k--\"), (th1b, \"k--\")):\n",
    "    plt.plot([split, split], [-0.05, 0.25], style, linewidth=2)\n",
    "plt.text(th0, 0.16, \"Depth=0\", fontsize=15)\n",
    "plt.text(th1a + 0.01, -0.01, \"Depth=1\", horizontalalignment=\"center\", fontsize=13)\n",
    "plt.text(th1b + 0.01, -0.01, \"Depth=1\", fontsize=13)\n",
    "plt.ylabel(\"$y$\", rotation=0)\n",
    "plt.legend(loc=\"upper center\", fontsize=16)\n",
    "plt.title(\"max_depth=2\")\n",
    "\n",
    "plt.sca(axes[1])\n",
    "th2s = tree_reg2.tree_.threshold[[2, 5, 9, 12]]\n",
    "plot_regression_predictions(tree_reg2, X_quad, y_quad)\n",
    "for split, style in ((th0, \"k-\"), (th1a, \"k--\"), (th1b, \"k--\")):\n",
    "    plt.plot([split, split], [-0.05, 0.25], style, linewidth=2)\n",
    "for split in th2s:\n",
    "    plt.plot([split, split], [-0.05, 0.25], \"k:\", linewidth=1)\n",
    "plt.text(th2s[2] + 0.01, 0.15, \"Depth=2\", fontsize=13)\n",
    "plt.title(\"max_depth=3\")\n",
    "\n",
    "save_fig(\"tree_regression_plot\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05fe9ebe",
   "metadata": {},
   "source": [
    "Note that under each region, the predicted value will be the average target value of the instances in the region (since that will be the value that minimizes the sum of squared errors)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f76966e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extra code – this cell generates and saves Figure 6–6\n",
    "\n",
    "tree_reg1 = DecisionTreeRegressor(random_state=42)\n",
    "tree_reg2 = DecisionTreeRegressor(random_state=42, min_samples_leaf=10)\n",
    "tree_reg1.fit(X_quad, y_quad)\n",
    "tree_reg2.fit(X_quad, y_quad)\n",
    "\n",
    "x1 = np.linspace(-0.5, 0.5, 500).reshape(-1, 1)\n",
    "y_pred1 = tree_reg1.predict(x1)\n",
    "y_pred2 = tree_reg2.predict(x1)\n",
    "\n",
    "fig, axes = plt.subplots(ncols=2, figsize=(10, 4), sharey=True)\n",
    "\n",
    "plt.sca(axes[0])\n",
    "plt.plot(X_quad, y_quad, \"b.\")\n",
    "plt.plot(x1, y_pred1, \"r.-\", linewidth=2, label=r\"$\\hat{y}$\")\n",
    "plt.axis([-0.5, 0.5, -0.05, 0.25])\n",
    "plt.xlabel(\"$x_1$\")\n",
    "plt.ylabel(\"$y$\", rotation=0)\n",
    "plt.legend(loc=\"upper center\")\n",
    "plt.title(\"No restrictions\")\n",
    "\n",
    "plt.sca(axes[1])\n",
    "plt.plot(X_quad, y_quad, \"b.\")\n",
    "plt.plot(x1, y_pred2, \"r.-\", linewidth=2, label=r\"$\\hat{y}$\")\n",
    "plt.axis([-0.5, 0.5, -0.05, 0.25])\n",
    "plt.xlabel(\"$x_1$\")\n",
    "plt.title(f\"min_samples_leaf={tree_reg2.min_samples_leaf}\")\n",
    "\n",
    "save_fig(\"tree_regression_regularization_plot\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9549dbcb",
   "metadata": {},
   "source": [
    "Note that regression trees are prone to overfit if there are no regularization!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c65d72f",
   "metadata": {},
   "source": [
    "# Sensitivity to Axis Orientation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b22cf0f4",
   "metadata": {},
   "source": [
    "Note that decisiont trees are only able to make boundaries using orthogonal boundaries, and hence rotating the axis of data can easily influence the decision boundary for the data.\n",
    "\n",
    "Hence, we can limit this by scaling the data, and then apply the principal component analysis transformation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b73707e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "pca_pipeline = make_pipeline(StandardScaler(), PCA())\n",
    "X_iris_rotated = pca_pipeline.fit_transform(X_iris)\n",
    "tree_clf_pca = DecisionTreeClassifier(max_depth=2, random_state=42)\n",
    "tree_clf_pca.fit(X_iris_rotated, y_iris)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55f8751c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extra code – this cell generates and saves Figure 6–8\n",
    "\n",
    "plt.figure(figsize=(8, 4))\n",
    "\n",
    "axes = [-2.2, 2.4, -0.6, 0.7]\n",
    "z0s, z1s = np.meshgrid(np.linspace(axes[0], axes[1], 100),\n",
    "                       np.linspace(axes[2], axes[3], 100))\n",
    "X_iris_pca_all = np.c_[z0s.ravel(), z1s.ravel()]\n",
    "y_pred = tree_clf_pca.predict(X_iris_pca_all).reshape(z0s.shape)\n",
    "\n",
    "plt.contourf(z0s, z1s, y_pred, alpha=0.3, cmap=custom_cmap)\n",
    "for idx, (name, style) in enumerate(zip(iris.target_names, (\"yo\", \"bs\", \"g^\"))):\n",
    "    plt.plot(X_iris_rotated[:, 0][y_iris == idx],\n",
    "             X_iris_rotated[:, 1][y_iris == idx],\n",
    "             style, label=f\"Iris {name}\")\n",
    "\n",
    "plt.xlabel(\"$z_1$\")\n",
    "plt.ylabel(\"$z_2$\", rotation=0)\n",
    "th1, th2 = tree_clf_pca.tree_.threshold[[0, 2]]\n",
    "plt.plot([th1, th1], axes[2:], \"k-\", linewidth=2)\n",
    "plt.plot([th2, th2], axes[2:], \"k--\", linewidth=2)\n",
    "plt.text(th1 - 0.01, axes[2] + 0.05, \"Depth=0\",\n",
    "         horizontalalignment=\"right\", fontsize=15)\n",
    "plt.text(th2 - 0.01, axes[2] + 0.05, \"Depth=1\",\n",
    "         horizontalalignment=\"right\", fontsize=13)\n",
    "plt.axis(axes)\n",
    "plt.legend(loc=(0.32, 0.67))\n",
    "save_fig(\"pca_preprocessing_plot\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d3b706d",
   "metadata": {},
   "source": [
    "# High Variance of Decision Trees"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c36ad0b2",
   "metadata": {},
   "source": [
    "Note that due to the nature of how the decision tree algorithm is used by Scikit-learn, retraining the same decision tree on the exact same data may produce very different models. hence, this motivates the prediction method by averaging over many tress to significantly reduce variance (e.g. random forest, boosting, etc)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee151214",
   "metadata": {},
   "source": [
    "# Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60c953bf",
   "metadata": {},
   "source": [
    "## 1. What is the approximate depth of a decision tree trained (without restrictions) on a training set with one million instances?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d83cf4b9",
   "metadata": {},
   "source": [
    "$log_2$(1,000,000) = 19.9 -> 20 depth"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "831fdb54",
   "metadata": {},
   "source": [
    "## 2. Is a node’s Gini impurity generally lower or higher than its parent’s? Is it generally lower/higher, or always lower/higher?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "008dc590",
   "metadata": {},
   "source": [
    "A node's gini impurity should generally be lower than its parents as the decision tree will try and classify towards 1 class as much as possible. Generally it is lower, but it might not necessarily be ALWAYS higher."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f7b6ec5",
   "metadata": {},
   "source": [
    "## 3. If a decision tree is overfitting the training set, is it a good idea to try decreasing max_depth?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6990f0a7",
   "metadata": {},
   "source": [
    "TRUE, a decision tree that is overfitted wil mean that the depth of the tree is too high, hence decreasing max_depth can be a good idea to overcome this issue."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbd07680",
   "metadata": {},
   "source": [
    "## 4. If a decision tree is underfitting the training set, is it a good idea to try scaling the input features?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ace49ebc",
   "metadata": {},
   "source": [
    "The scale of the features will not matter, the relative order / rank will matter."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "309be32c",
   "metadata": {},
   "source": [
    "## 5. If it takes one hour to train a decision tree on a training set containing one million instances, roughly how much time will it take to train another decision tree on a training set containing ten million instances? Hint: consider the CART algorithm’s computational complexity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55aa6b3f",
   "metadata": {},
   "source": [
    "The computational complexity of training a Decision Tree is _O_(_n_ × _m_ log₂(_m_)). So if you multiply the training set size by 10, the training time will be multiplied by _K_ = (_n_ × 10 _m_ × log₂(10 _m_)) / (_n_ × _m_ × log₂(_m_)) = 10 × log₂(10 _m_) / log₂(_m_).\n",
    "\n",
    "\n",
    "If _m_ = 10<sup>6</sup>, then _K_ ≈ 11.7, so you can expect the training time to be roughly 11.7 hours."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d09821df",
   "metadata": {},
   "source": [
    "## 6. If it takes one hour to train a decision tree on a given training set, roughly how much time will it take if you double the number of features?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0d54d45",
   "metadata": {},
   "source": [
    "If the number of features double, then the time it takes to train the new decision tree will be double."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0f7e93a",
   "metadata": {},
   "source": [
    "## 7. Train and fine-tune a decision tree for the moons dataset by following these steps:\n",
    "1. Use make_moons(n_samples=10000, noise=0.4) to generate a moons dataset.\n",
    "2. Use train_test_split() to split the dataset into a training set and a test set.\n",
    "3. Use grid search with cross-validation (with the help of the GridSearchCV class) to find good hyperparameter values for a DecisionTreeClassifier. Hint: try various values for max_leaf_nodes.\n",
    "4. Train it on the full training set using these hyperparameters, and measure your model’s performance on the test set. You should get roughly 85% to 87% accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f5d7a71",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_moons\n",
    "X_moons, y_moons = make_moons (n_samples = 10000, noise = 0.4, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "488efa6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_moons, y_moons, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef20b3d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param = {\n",
    "    \"max_leaf_nodes\": list(range(10,100)),\n",
    "}\n",
    "\n",
    "grid_search_dt = GridSearchCV(DecisionTreeClassifier(max_leaf_nodes= 10, random_state=42),\n",
    "                         param,\n",
    "                         cv=10,\n",
    "                         scoring=\"roc_auc\"\n",
    ")\n",
    "grid_search_dt.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a415ecb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = grid_search_dt.best_estimator_.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90d4a2f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "roc_auc_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "892c9a72",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "038eb354",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extra code – this cell generates and saves Figure 6–3\n",
    "\n",
    "def plot_decision_boundary(clf, X, y, axes, cmap):\n",
    "    x1, x2 = np.meshgrid(np.linspace(axes[0], axes[1], 100),\n",
    "                         np.linspace(axes[2], axes[3], 100))\n",
    "    X_new = np.c_[x1.ravel(), x2.ravel()]\n",
    "    y_pred = clf.predict(X_new).reshape(x1.shape)\n",
    "    \n",
    "    plt.contourf(x1, x2, y_pred, alpha=0.3, cmap=cmap)\n",
    "    plt.contour(x1, x2, y_pred, cmap=\"Greys\", alpha=0.8)\n",
    "    colors = {\"Wistia\": [\"#78785c\", \"#c47b27\"], \"Pastel1\": [\"red\", \"blue\"]}\n",
    "    markers = (\"o\", \"^\")\n",
    "    for idx in (0, 1):\n",
    "        plt.plot(X[:, 0][y == idx], X[:, 1][y == idx],\n",
    "                 color=colors[cmap][idx], marker=markers[idx], linestyle=\"none\")\n",
    "    plt.axis(axes)\n",
    "    plt.xlabel(r\"$x_1$\")\n",
    "    plt.ylabel(r\"$x_2$\", rotation=0)\n",
    "\n",
    "# fig, axes = plt.subplots(ncols=2, figsize=(10, 4), sharey=True)\n",
    "# plt.sca(axes[0])\n",
    "plot_decision_boundary(grid_search_dt.best_estimator_, X_test, y_test,\n",
    "                       axes=[-1.5, 2.4, -1, 1.5], cmap=\"Wistia\")\n",
    "plt.title(\"Decision Tree Boundary based on Testing Dataset\")\n",
    "# plt.sca(axes[1])\n",
    "# plot_decision_boundary(tree_clf2, X_moons, y_moons,\n",
    "#                        axes=[-1.5, 2.4, -1, 1.5], cmap=\"Wistia\")\n",
    "# plt.title(f\"min_samples_leaf = {tree_clf2.min_samples_leaf}\")\n",
    "# plt.ylabel(\"\")\n",
    "# save_fig(\"min_samples_leaf_plot\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20c3e0a4",
   "metadata": {},
   "source": [
    "## 8. Grow a forest by following these steps:\n",
    "1. Continuing the previous exercise, generate 1,000 subsets of the training set, each containing 100 instances selected randomly. Hint: you can use Scikit-Learn’s ShuffleSplit class for this.\n",
    "2. Train one decision tree on each subset, using the best hyperparameter values found in the previous exercise. Evaluate these 1,000 decision trees on the test set. Since they were trained on smaller sets, these decision trees will likely perform worse than the first decision tree, achieving only about 80% accuracy.\n",
    "3. Now comes the magic. For each test set instance, generate the predictions of the 1,000 decision trees, and keep only the most frequent prediction (you can use SciPy’s mode() function for this). This approach gives you majority-vote predictions over the test set.\n",
    "4. Evaluate these predictions on the test set: you should obtain a slightly higher accuracy than your first model (about 0.5 to 1.5% higher). Congratulations, you have trained a random forest classifier!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbc1b2d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate 1,000 subsets each containing 100 instances (0.0125 of the training instance)\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "\n",
    "rs = ShuffleSplit(n_splits=1000, train_size = 0.0125, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec6f0d5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train a decision tree on each subset using the max_leaf_nodes parameter obtained previously\n",
    "acc_list = []\n",
    "\n",
    "# create an arbitrary valued array with size 1000, len(X_test)\n",
    "# this is where we will store our prediction for each of the X_test data\n",
    "Y_pred = np.empty([1000, len(X_test)], dtype=np.uint8)\n",
    "\n",
    "for i, (train_index, test_index) in enumerate(rs.split(X_train)):\n",
    "    tree_clf = DecisionTreeClassifier(max_leaf_nodes=grid_search_dt.best_estimator_.get_params()[\"max_leaf_nodes\"])\n",
    "    tree_clf.fit(X_train[train_index], y_train[train_index])\n",
    "    y_pred = tree_clf.predict(X_test)\n",
    "    acc_score = accuracy_score(y_test, y_pred)\n",
    "    acc_list.append(acc_score)\n",
    "    # set Y_pred value as y_pred\n",
    "    Y_pred[i] = y_pred\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11898010",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the mean of the accuracy scores in acc_list, indeed it is around 80% accuracy\n",
    "np.mean(acc_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad55fd62",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import mode\n",
    "# compute the most frqeuently appearing predicted class, note the axis = 0 means across all the rows\n",
    "# aka mode for each column\n",
    "# the first value is the majority vote value, the second value is the total count of said value\n",
    "y_pred_majority_votes, n_votes = mode(Y_pred, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2ae25aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(y_test, y_pred_majority_votes.reshape([-1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a674991",
   "metadata": {},
   "source": [
    "Above we can see that indeed by using 1000 trees, we can have similar level of accuracy when we only have a limited amount of data!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d73f9085",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
