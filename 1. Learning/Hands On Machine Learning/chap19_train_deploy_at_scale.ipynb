{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9cd2b53d",
   "metadata": {},
   "source": [
    "# Training and Deploying TensorFlow Models at Scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "138d07af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "assert sys.version_info >= (3, 7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bdce3dbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"TF_USE_LEGACY_KERAS\"] = \"1\"\n",
    "import tf_keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9163e7e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from packaging import version\n",
    "import tensorflow as tf\n",
    "\n",
    "assert version.parse(tf.__version__) >= version.parse(\"2.8.0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "19540f84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No GPU was detected. Neural nets can be very slow without a GPU.\n"
     ]
    }
   ],
   "source": [
    "if not tf.config.list_physical_devices('GPU'):\n",
    "    print(\"No GPU was detected. Neural nets can be very slow without a GPU.\")\n",
    "    if \"google.colab\" in sys.modules:\n",
    "        print(\"Go to Runtime > Change runtime and select a GPU hardware \"\n",
    "              \"accelerator.\")\n",
    "    if \"kaggle_secrets\" in sys.modules:\n",
    "        print(\"Go to Settings > Accelerator and select GPU.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1646ecb",
   "metadata": {},
   "source": [
    "## Serving a TensorFlow Model\n",
    "\n",
    "While calling a `predict()` method can be done early on, as the infrastructure grows there comes a point where it is preferable to wrap the model in a small service whose sole role is to make predictions and have the rest of the infrastructure query it (via a REST or gRPC API). This allows easily switching model versions or scaling the service up as needed, perform A/B experiments and ensure that all software components rely on the same model versions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b63b97c",
   "metadata": {},
   "source": [
    "### Using TensorFlow Serving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1368d5d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import tensorflow as tf\n",
    "\n",
    "# extra code – load and split the MNIST dataset\n",
    "mnist = tf.keras.datasets.mnist.load_data()\n",
    "(X_train_full, y_train_full), (X_test, y_test) = mnist\n",
    "X_valid, X_train = X_train_full[:5000], X_train_full[5000:]\n",
    "y_valid, y_train = y_train_full[:5000], y_train_full[5000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cde07755",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# extra code – build & train an MNIST model (also handles image preprocessing)\n",
    "tf.random.set_seed(42)\n",
    "tf.keras.backend.clear_session()\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Flatten(input_shape=[28, 28], dtype=tf.uint8),\n",
    "    tf.keras.layers.Rescaling(scale=1 / 255),\n",
    "    tf.keras.layers.Dense(100, activation=\"relu\"),\n",
    "    tf.keras.layers.Dense(10, activation=\"softmax\")\n",
    "])\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "              optimizer=tf.keras.optimizers.SGD(learning_rate=1e-2),\n",
    "              metrics=[\"accuracy\"])\n",
    "model.fit(X_train, y_train, epochs=10, validation_data=(X_valid, y_valid))\n",
    "\n",
    "model_name = \"my_mnist_model\"\n",
    "model_version = \"0001\"\n",
    "model_path = Path(model_name) / model_version\n",
    "model.save(model_path, save_format=\"tf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8cce7217",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " flatten (Flatten)           (None, 784)               0         \n",
      "                                                                 \n",
      " rescaling (Rescaling)       (None, 784)               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 100)               78500     \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 10)                1010      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 79510 (310.59 KB)\n",
      "Trainable params: 79510 (310.59 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aa900ae",
   "metadata": {},
   "source": [
    "Let's take a look at the file tree (we've discussed what each of these file is used for in chapter 10):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4bc296f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['my_mnist_model/0001',\n",
       " 'my_mnist_model/0001/assets',\n",
       " 'my_mnist_model/0001/fingerprint.pb',\n",
       " 'my_mnist_model/0001/keras_metadata.pb',\n",
       " 'my_mnist_model/0001/saved_model.pb',\n",
       " 'my_mnist_model/0001/variables',\n",
       " 'my_mnist_model/0001/variables/variables.data-00000-of-00001',\n",
       " 'my_mnist_model/0001/variables/variables.index']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted([str(path) for path in model_path.parent.glob(\"**/*\")])  # extra code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "400f19b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The given SavedModel contains the following tag-sets:\n",
      "'serve'\n"
     ]
    }
   ],
   "source": [
    "!saved_model_cli show --dir '{model_path}'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba561012",
   "metadata": {},
   "source": [
    "A SavedModel contains one or more *metagraphs*, which is a computation graph plus some function signature definitions, including their input and output names, types and shapes. Each metagraph is identified by a set of tags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "237772d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The given SavedModel MetaGraphDef contains SignatureDefs with the following keys:\n",
      "SignatureDef key: \"__saved_model_init_op\"\n",
      "SignatureDef key: \"serving_default\"\n"
     ]
    }
   ],
   "source": [
    "!saved_model_cli show --dir '{model_path}' --tag_set serve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1097ad10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The given SavedModel SignatureDef contains the following input(s):\n",
      "  inputs['flatten_input'] tensor_info:\n",
      "      dtype: DT_UINT8\n",
      "      shape: (-1, 28, 28)\n",
      "      name: serving_default_flatten_input:0\n",
      "The given SavedModel SignatureDef contains the following output(s):\n",
      "  outputs['dense_1'] tensor_info:\n",
      "      dtype: DT_FLOAT\n",
      "      shape: (-1, 10)\n",
      "      name: StatefulPartitionedCall:0\n",
      "Method name is: tensorflow/serving/predict\n"
     ]
    }
   ],
   "source": [
    "!saved_model_cli show --dir '{model_path}' --tag_set serve \\\n",
    "                      --signature_def serving_default"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aea2222",
   "metadata": {},
   "source": [
    "### Installing and starting TensorFlow Serving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "007d857e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash --bg\n",
    "docker pull tensorflow/serving  # downloads the latest TF Serving image\n",
    "\n",
    "docker run -it --rm -v \"/Users/kevinkyhalim/ML_repo/1. Learning/Hands On Machine Learning/my_mnist_model:/models/my_mnist_model\" \\\n",
    "    -p 8500:8500 -p 8501:8501 -e MODEL_NAME=my_mnist_model tensorflow/serving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "8b9a801d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "time.sleep(2) # let's wait a couple seconds for the server to start"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfb91777",
   "metadata": {},
   "source": [
    "### Querying TF Serving through the REST API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1e6e03c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "X_new = X_test[:3]  # pretend we have 3 new digit images to classify\n",
    "request_json = json.dumps({\n",
    "    \"signature_name\": \"serving_default\",\n",
    "    \"instances\": X_new.tolist(),\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "208863a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\"signature_name\": \"serving_default\", \"instances\": [[[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0..., 0, 0]]]}'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "request_json[:100] + \"...\" + request_json[-10:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f567c9c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "server_url = \"http://localhost:8501/v1/models/my_mnist_model:predict\"\n",
    "response = requests.post(server_url, data=request_json)\n",
    "response.raise_for_status()  # raise an exception in case of error\n",
    "response = response.json()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a66f5333",
   "metadata": {},
   "source": [
    "### Querying TF Serving through the gRPC API\n",
    "\n",
    "(requires a local server to be run, specifically tensorflow/serving)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "45f3a1af",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow_serving.apis.predict_pb2 import PredictRequest\n",
    "\n",
    "model_name = \"my_mnist_model\"\n",
    "\n",
    "request = PredictRequest()\n",
    "request.model_spec.name = model_name\n",
    "request.model_spec.signature_name = \"serving_default\"\n",
    "input_name = model.input_names[0]  # == \"flatten_input\"\n",
    "request.inputs[input_name].CopyFrom(tf.make_tensor_proto(X_new))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08bdf536",
   "metadata": {},
   "outputs": [],
   "source": [
    "import grpc\n",
    "from tensorflow_serving.apis import prediction_service_pb2_grpc\n",
    "\n",
    "channel = grpc.insecure_channel('localhost:8500')\n",
    "predict_service = prediction_service_pb2_grpc.PredictionServiceStub(channel)\n",
    "response = predict_service.Predict(request, timeout=10.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea953392",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_name = model.output_names[0]\n",
    "outputs_proto = response.outputs[output_name]\n",
    "y_proba = tf.make_ndarray(outputs_proto)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eea2f8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_proba.round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95b7affb",
   "metadata": {},
   "source": [
    "If your client does not include the TensorFlow library, you can convert the response to a NumPy array like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbfaa7c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extra code – shows how to avoid using tf.make_ndarray()\n",
    "output_name = model.output_names[0]\n",
    "outputs_proto = response.outputs[output_name]\n",
    "shape = [dim.size for dim in outputs_proto.tensor_shape.dim]\n",
    "y_proba = np.array(outputs_proto.float_val).reshape(shape)\n",
    "y_proba.round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7050243c",
   "metadata": {},
   "source": [
    "### Deploying a new model version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d0a25ac7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.SGD` runs slowly on M1/M2 Macs, please use the legacy TF-Keras optimizer instead, located at `tf.keras.optimizers.legacy.SGD`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1719/1719 [==============================] - 1s 369us/step - loss: 0.7080 - accuracy: 0.8098 - val_loss: 0.3514 - val_accuracy: 0.9044\n",
      "Epoch 2/10\n",
      "1719/1719 [==============================] - 1s 378us/step - loss: 0.3266 - accuracy: 0.9066 - val_loss: 0.2773 - val_accuracy: 0.9218\n",
      "Epoch 3/10\n",
      "1719/1719 [==============================] - 1s 413us/step - loss: 0.2746 - accuracy: 0.9204 - val_loss: 0.2383 - val_accuracy: 0.9332\n",
      "Epoch 4/10\n",
      "1719/1719 [==============================] - 1s 355us/step - loss: 0.2400 - accuracy: 0.9308 - val_loss: 0.2141 - val_accuracy: 0.9396\n",
      "Epoch 5/10\n",
      "1719/1719 [==============================] - 1s 358us/step - loss: 0.2145 - accuracy: 0.9388 - val_loss: 0.1929 - val_accuracy: 0.9430\n",
      "Epoch 6/10\n",
      "1719/1719 [==============================] - 1s 345us/step - loss: 0.1950 - accuracy: 0.9434 - val_loss: 0.1784 - val_accuracy: 0.9498\n",
      "Epoch 7/10\n",
      "1719/1719 [==============================] - 1s 361us/step - loss: 0.1791 - accuracy: 0.9481 - val_loss: 0.1651 - val_accuracy: 0.9540\n",
      "Epoch 8/10\n",
      "1719/1719 [==============================] - 1s 349us/step - loss: 0.1662 - accuracy: 0.9518 - val_loss: 0.1637 - val_accuracy: 0.9530\n",
      "Epoch 9/10\n",
      "1719/1719 [==============================] - 1s 342us/step - loss: 0.1549 - accuracy: 0.9557 - val_loss: 0.1506 - val_accuracy: 0.9580\n",
      "Epoch 10/10\n",
      "1719/1719 [==============================] - 1s 370us/step - loss: 0.1460 - accuracy: 0.9580 - val_loss: 0.1414 - val_accuracy: 0.9606\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "# extra code – build and train a new MNIST model version\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Flatten(input_shape=[28, 28], dtype=tf.uint8),\n",
    "    tf.keras.layers.Rescaling(scale=1 / 255),\n",
    "    tf.keras.layers.Dense(50, activation=\"relu\"),\n",
    "    tf.keras.layers.Dense(50, activation=\"relu\"),\n",
    "    tf.keras.layers.Dense(10, activation=\"softmax\")\n",
    "])\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "              optimizer=tf.keras.optimizers.SGD(learning_rate=1e-2),\n",
    "              metrics=[\"accuracy\"])\n",
    "history = model.fit(X_train, y_train, epochs=10,\n",
    "                    validation_data=(X_valid, y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "dff838d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: my_mnist_model/0002/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: my_mnist_model/0002/assets\n"
     ]
    }
   ],
   "source": [
    "model_version = \"0002\"\n",
    "model_path = Path(model_name) / model_version\n",
    "model.save(model_path, save_format=\"tf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "470f8ad8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['my_mnist_model/0001',\n",
       " 'my_mnist_model/0001/assets',\n",
       " 'my_mnist_model/0001/fingerprint.pb',\n",
       " 'my_mnist_model/0001/keras_metadata.pb',\n",
       " 'my_mnist_model/0001/saved_model.pb',\n",
       " 'my_mnist_model/0001/variables',\n",
       " 'my_mnist_model/0001/variables/variables.data-00000-of-00001',\n",
       " 'my_mnist_model/0001/variables/variables.index',\n",
       " 'my_mnist_model/0002',\n",
       " 'my_mnist_model/0002/assets',\n",
       " 'my_mnist_model/0002/fingerprint.pb',\n",
       " 'my_mnist_model/0002/keras_metadata.pb',\n",
       " 'my_mnist_model/0002/saved_model.pb',\n",
       " 'my_mnist_model/0002/variables',\n",
       " 'my_mnist_model/0002/variables/variables.data-00000-of-00001',\n",
       " 'my_mnist_model/0002/variables/variables.index']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted([str(path) for path in model_path.parent.glob(\"**/*\")])  # extra code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3f696f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "server_url = \"http://localhost:8501/v1/models/my_mnist_model:predict\"\n",
    "            \n",
    "response = requests.post(server_url, data=request_json)\n",
    "response.raise_for_status()\n",
    "response = response.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d10b13b",
   "metadata": {},
   "outputs": [],
   "source": [
    "response.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d129d49d",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_proba = np.array(response[\"predictions\"])\n",
    "y_proba.round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06afb179",
   "metadata": {},
   "source": [
    "### Creating a Prediction Service on Vertex AI\n",
    "\n",
    "Vertex AI is a platform within GCP that offers a wide range of AI-related tools and services. We can upload datasets, get humans to label them, store commonly used features in a feature store and use them for training or in production and train models across many GPU / TPU servers with automatic hyperparameter tuning or model architecture search (AutoML)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e106ed63",
   "metadata": {},
   "outputs": [],
   "source": [
    "project_id = \"lucid-bond-463306-k8\"  ##### CHANGE THIS TO YOUR PROJECT ID #####\n",
    "\n",
    "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = \"lucid-bond-463306-k8-1062e758e5e5.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f539623e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import storage\n",
    "\n",
    "bucket_name = \"mnist_model_bucket\"  ##### CHANGE THIS TO A UNIQUE BUCKET NAME #####\n",
    "location = \"us-central1\"\n",
    "\n",
    "storage_client = storage.Client(project=project_id)\n",
    "# bucket = storage_client.create_bucket(bucket_name, location=location)\n",
    "bucket = storage_client.bucket(bucket_name)  # to reuse a bucket instead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "eacce2e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def upload_directory(bucket, dirpath):\n",
    "    dirpath = Path(dirpath)\n",
    "    for filepath in dirpath.glob(\"**/*\"):\n",
    "        if filepath.is_file():\n",
    "            blob = bucket.blob(filepath.relative_to(dirpath.parent).as_posix())\n",
    "            blob.upload_from_filename(filepath)\n",
    "\n",
    "upload_directory(bucket, \"my_mnist_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1f41b9ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extra code – a much faster multithreaded implementation of upload_directory()\n",
    "#              which also accepts a prefix for the target path, and prints stuff\n",
    "\n",
    "from concurrent import futures\n",
    "\n",
    "def upload_file(bucket, filepath, blob_path):\n",
    "    blob = bucket.blob(blob_path)\n",
    "    blob.upload_from_filename(filepath)\n",
    "\n",
    "def upload_directory(bucket, dirpath, prefix=None, max_workers=50):\n",
    "    dirpath = Path(dirpath)\n",
    "    prefix = prefix or dirpath.name\n",
    "    with futures.ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        future_to_filepath = {\n",
    "            executor.submit(\n",
    "                upload_file,\n",
    "                bucket, filepath,\n",
    "                f\"{prefix}/{filepath.relative_to(dirpath).as_posix()}\"\n",
    "            ): filepath\n",
    "            for filepath in sorted(dirpath.glob(\"**/*\"))\n",
    "            if filepath.is_file()\n",
    "        }\n",
    "        for future in futures.as_completed(future_to_filepath):\n",
    "            filepath = future_to_filepath[future]\n",
    "            try:\n",
    "                result = future.result()\n",
    "            except Exception as ex:\n",
    "                print(f\"Error uploading {filepath!s:60}: {ex}\")  # f!s is str(f)\n",
    "            else:\n",
    "                print(f\"Uploaded {filepath!s:60}\", end=\"\\r\")\n",
    "\n",
    "    print(f\"Uploaded {dirpath!s:60}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7f15279e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating Model\n",
      "Create Model backing LRO: projects/954975238569/locations/us-central1/models/681559770267648000/operations/3461124993486684160\n",
      "Model created. Resource name: projects/954975238569/locations/us-central1/models/681559770267648000@1\n",
      "To use this Model in another session:\n",
      "model = aiplatform.Model('projects/954975238569/locations/us-central1/models/681559770267648000@1')\n"
     ]
    }
   ],
   "source": [
    "from google.cloud import aiplatform\n",
    "\n",
    "server_image = \"gcr.io/cloud-aiplatform/prediction/tf2-gpu.2-8:latest\"\n",
    "\n",
    "aiplatform.init(project=project_id, location=location)\n",
    "mnist_model = aiplatform.Model.upload(\n",
    "    display_name=\"mnist\",\n",
    "    artifact_uri=f\"gs://{bucket_name}/my_mnist_model/0001\",\n",
    "    serving_container_image_uri=server_image,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a725d0f",
   "metadata": {},
   "source": [
    "Create an *endpoint*, where the client applications connect to when they want to access a service, then we need to deploy the model to this endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8c429eeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating Endpoint\n",
      "Create Endpoint backing LRO: projects/954975238569/locations/us-central1/endpoints/8770669014838935552/operations/1401854063871524864\n",
      "Endpoint created. Resource name: projects/954975238569/locations/us-central1/endpoints/8770669014838935552\n",
      "To use this Endpoint in another session:\n",
      "endpoint = aiplatform.Endpoint('projects/954975238569/locations/us-central1/endpoints/8770669014838935552')\n",
      "Deploying Model projects/954975238569/locations/us-central1/models/681559770267648000 to Endpoint : projects/954975238569/locations/us-central1/endpoints/8770669014838935552\n",
      "Deploy Endpoint model backing LRO: projects/954975238569/locations/us-central1/endpoints/8770669014838935552/operations/2740549053107404800\n",
      "Endpoint model deployed. Resource name: projects/954975238569/locations/us-central1/endpoints/8770669014838935552\n"
     ]
    }
   ],
   "source": [
    "endpoint = aiplatform.Endpoint.create(display_name=\"mnist-endpoint\")\n",
    "\n",
    "endpoint.deploy(\n",
    "    mnist_model,\n",
    "    min_replica_count=1,\n",
    "    # this number will depend on how many accelerator counts are \n",
    "    # allowed to have under the specific GPU\n",
    "    max_replica_count=1,\n",
    "    machine_type=\"n1-standard-4\",\n",
    "    accelerator_type=\"NVIDIA_TESLA_T4\",\n",
    "    accelerator_count=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "12973c03",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = endpoint.predict(instances=X_new.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a917ed44",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 1.  , 0.  , 0.  ],\n",
       "       [0.  , 0.  , 0.97, 0.02, 0.  , 0.  , 0.  , 0.  , 0.  , 0.  ],\n",
       "       [0.  , 0.98, 0.01, 0.  , 0.  , 0.  , 0.  , 0.01, 0.  , 0.  ]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "np.round(response.predictions, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a1dc9194",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Undeploying Endpoint model: projects/954975238569/locations/us-central1/endpoints/8770669014838935552\n",
      "Undeploy Endpoint model backing LRO: projects/954975238569/locations/us-central1/endpoints/8770669014838935552/operations/7790773085249994752\n",
      "Endpoint model undeployed. Resource name: projects/954975238569/locations/us-central1/endpoints/8770669014838935552\n",
      "Deleting Endpoint : projects/954975238569/locations/us-central1/endpoints/8770669014838935552\n",
      "Endpoint deleted. . Resource name: projects/954975238569/locations/us-central1/endpoints/8770669014838935552\n",
      "Deleting Endpoint resource: projects/954975238569/locations/us-central1/endpoints/8770669014838935552\n",
      "Delete Endpoint backing LRO: projects/954975238569/locations/us-central1/operations/6545527788282052608\n",
      "Endpoint resource projects/954975238569/locations/us-central1/endpoints/8770669014838935552 deleted.\n"
     ]
    }
   ],
   "source": [
    "endpoint.undeploy_all()  # undeploy all models from the endpoint\n",
    "endpoint.delete()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7410e3f7",
   "metadata": {},
   "source": [
    "## Running Batch Prediction Jobs on Vertex AI "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "de595e16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploaded my_mnist_batch                                              \n"
     ]
    }
   ],
   "source": [
    "batch_path = Path(\"my_mnist_batch\")\n",
    "batch_path.mkdir(exist_ok=True)\n",
    "with open(batch_path / \"my_mnist_batch.jsonl\", \"w\") as jsonl_file:\n",
    "    for image in X_test[:100].tolist():\n",
    "        jsonl_file.write(json.dumps(image))\n",
    "        jsonl_file.write(\"\\n\")\n",
    "\n",
    "upload_directory(bucket, batch_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8b8f623d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating BatchPredictionJob\n",
      "BatchPredictionJob created. Resource name: projects/954975238569/locations/us-central1/batchPredictionJobs/6587159748095770624\n",
      "To use this BatchPredictionJob in another session:\n",
      "bpj = aiplatform.BatchPredictionJob('projects/954975238569/locations/us-central1/batchPredictionJobs/6587159748095770624')\n",
      "View Batch Prediction Job:\n",
      "https://console.cloud.google.com/ai/platform/locations/us-central1/batch-predictions/6587159748095770624?project=954975238569\n",
      "BatchPredictionJob projects/954975238569/locations/us-central1/batchPredictionJobs/6587159748095770624 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "BatchPredictionJob projects/954975238569/locations/us-central1/batchPredictionJobs/6587159748095770624 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "BatchPredictionJob projects/954975238569/locations/us-central1/batchPredictionJobs/6587159748095770624 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "BatchPredictionJob projects/954975238569/locations/us-central1/batchPredictionJobs/6587159748095770624 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "BatchPredictionJob projects/954975238569/locations/us-central1/batchPredictionJobs/6587159748095770624 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "BatchPredictionJob projects/954975238569/locations/us-central1/batchPredictionJobs/6587159748095770624 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "BatchPredictionJob projects/954975238569/locations/us-central1/batchPredictionJobs/6587159748095770624 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "BatchPredictionJob projects/954975238569/locations/us-central1/batchPredictionJobs/6587159748095770624 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "BatchPredictionJob projects/954975238569/locations/us-central1/batchPredictionJobs/6587159748095770624 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "BatchPredictionJob projects/954975238569/locations/us-central1/batchPredictionJobs/6587159748095770624 current state:\n",
      "JobState.JOB_STATE_SUCCEEDED\n",
      "BatchPredictionJob run completed. Resource name: projects/954975238569/locations/us-central1/batchPredictionJobs/6587159748095770624\n"
     ]
    }
   ],
   "source": [
    "batch_prediction_job = mnist_model.batch_predict(\n",
    "    job_display_name=\"my_batch_prediction_job\",\n",
    "    machine_type=\"n1-standard-4\",\n",
    "    starting_replica_count=1,\n",
    "    max_replica_count=1,\n",
    "    accelerator_type=\"NVIDIA_TESLA_T4\",\n",
    "    accelerator_count=1,\n",
    "    gcs_source=[f\"gs://{bucket_name}/{batch_path.name}/my_mnist_batch.jsonl\"],\n",
    "    gcs_destination_prefix=f\"gs://{bucket_name}/my_mnist_predictions/\",\n",
    "    sync=True  # set to False if you don't want to wait for completion\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "88ab2825",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "gcs_output_directory: \"gs://mnist_model_bucket/my_mnist_predictions/prediction-mnist-2025_06_18T00_19_59_803Z\""
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_prediction_job.output_info  # extra code – shows the output directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c9fb1c83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "my_mnist_predictions/prediction-mnist-2025_06_18T00_19_59_803Z/prediction.errors_stats-00000-of-00001\n",
      "my_mnist_predictions/prediction-mnist-2025_06_18T00_19_59_803Z/prediction.results-00000-of-00001\n"
     ]
    }
   ],
   "source": [
    "y_probas = []\n",
    "for blob in batch_prediction_job.iter_outputs():\n",
    "    print(blob.name)  # extra code\n",
    "    if \"prediction.results\" in blob.name:\n",
    "        for line in blob.download_as_text().splitlines():\n",
    "            y_proba = json.loads(line)[\"prediction\"]\n",
    "            y_probas.append(y_proba)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d47af2f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = np.argmax(y_probas, axis=1)\n",
    "accuracy = np.sum(y_pred == y_test[:100]) / 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "84daa91b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(0.98)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a17f2aad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleting Model : projects/954975238569/locations/us-central1/models/681559770267648000\n",
      "Model deleted. . Resource name: projects/954975238569/locations/us-central1/models/681559770267648000\n",
      "Deleting Model resource: projects/954975238569/locations/us-central1/models/681559770267648000\n",
      "Delete Model backing LRO: projects/954975238569/locations/us-central1/models/681559770267648000/operations/7771395292322070528\n",
      "Model resource projects/954975238569/locations/us-central1/models/681559770267648000 deleted.\n"
     ]
    }
   ],
   "source": [
    "mnist_model.delete()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0f1b70f",
   "metadata": {},
   "source": [
    "Let's delete all the directories we created on GCS (i.e., all the blobs with these prefixes):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "520ac24f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleting BatchPredictionJob : projects/954975238569/locations/us-central1/batchPredictionJobs/6587159748095770624\n",
      "BatchPredictionJob deleted. . Resource name: projects/954975238569/locations/us-central1/batchPredictionJobs/6587159748095770624\n",
      "Deleting BatchPredictionJob resource: projects/954975238569/locations/us-central1/batchPredictionJobs/6587159748095770624\n",
      "Delete BatchPredictionJob backing LRO: projects/954975238569/locations/us-central1/operations/553250989554008064\n",
      "BatchPredictionJob resource projects/954975238569/locations/us-central1/batchPredictionJobs/6587159748095770624 deleted.\n"
     ]
    }
   ],
   "source": [
    "for prefix in [\"my_mnist_model/\", \"my_mnist_batch/\", \"my_mnist_predictions/\"]:\n",
    "    blobs = bucket.list_blobs(prefix=prefix)\n",
    "    for blob in blobs:\n",
    "        blob.delete()\n",
    "\n",
    "#bucket.delete()  # uncomment and run if you want to delete the bucket itself\n",
    "batch_prediction_job.delete()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19323000",
   "metadata": {},
   "source": [
    "## Deploying a Model to a Mobile or Embedded Device\n",
    "\n",
    "*edge computing*, where machine learning models run closer to the source of data, for example in the user's mobile device or in an embedded device.\n",
    "\n",
    "Advantages include:\n",
    "1. Allows the device to be smart even when not connected to the internet;\n",
    "2. Reduces latency by not having to send data to a remote server\n",
    "3. Reduces the load on the servers\n",
    "4. May improve privacy since the user's data can stay on the device.\n",
    "\n",
    "Disadvantages:\n",
    "1. Computing resources are generally tiny compared to multi-GPU servers;\n",
    "2. A large model may not fit in the device\n",
    "3. May use too much RAM and CPU\n",
    "4. May take too long to download\n",
    "\n",
    "To overcome this issue, we can leverage the TFLite library that can\n",
    "1. Reduce the model size, shortening download time and reducing RAM usage\n",
    "2. Reduce the amount of computations needed for each predictions, reducing latency, battery usage and heating\n",
    "3. Adapt the model to device-specific constraints\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3dce135",
   "metadata": {},
   "outputs": [],
   "source": [
    "converter = tf.lite.TFLiteConverter.from_saved_model(str(model_path))\n",
    "tflite_model = converter.convert()\n",
    "with open(\"my_converted_savedmodel.tflite\", \"wb\") as f:\n",
    "    f.write(tflite_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c43e6577",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extra code – shows how to convert a Keras model\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9f16049",
   "metadata": {},
   "source": [
    "**Post training Quantization**\n",
    "\n",
    "Quantizing the weights after training (through symmetrical quantization technique, where it finds the maximum absolute value, *m*, then it maps the floating-point range -m to +m to the fixed-point (integer) range -127 to +127).\n",
    "\n",
    "Mostly used to reduce the application's size (since anyway the model needs to convert the quantized weights to floats)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58956fca",
   "metadata": {},
   "outputs": [],
   "source": [
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a89473d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "tflite_model = converter.convert()\n",
    "with open(\"my_converted_keras_model.tflite\", \"wb\") as f:\n",
    "    f.write(tflite_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17a95f09",
   "metadata": {},
   "source": [
    "Main issue with quantizaton is that it loses a bit of accuracy (similar to adding noise to the weights and activations), if the accuracy drop is too severe, then may need to use *quantization-aware training*, where it adds fake quantization operations to the model so it can learn to ignore the quantization noise during training, enabling the final model weights to be more robust to quantization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca747531",
   "metadata": {},
   "source": [
    "Check out the O’Reilly books **TinyML: Machine Learning with TensorFlow on Arduino and Ultra-Low Power Micro-Controllers**, by Pete Warden (former lead of the TFLite team) and Daniel Situnayake and **AI and Machine Learning for On-Device Development**, by Laurence Moroney."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa7e99df",
   "metadata": {},
   "source": [
    "## Running a Model in a Web Page"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2a8b8dd",
   "metadata": {},
   "source": [
    "Code examples for this section are hosted on glitch.com, a website that lets you create Web apps for free.\n",
    "\n",
    "* https://homl.info/tfjscode: a simple TFJS Web app that loads a pretrained model and classifies an image.\n",
    "* https://homl.info/tfjswpa: the same Web app setup as a WPA. Try opening this link on various platforms, including mobile devices.\n",
    "** https://homl.info/wpacode: this WPA's source code.\n",
    "* https://tensorflow.org/js: The TFJS library.\n",
    "** https://www.tensorflow.org/js/demos: some fun demos."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75e96ced",
   "metadata": {},
   "source": [
    "If you want to learn more about TensorFlow.js, check out the O’reilly books **Practical Deep Learning　for Cloud, Mobile, and Edge**, by Anirudh Koul et al., or **Learning TensorFlow.js**, by Gant Laborde."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99771dcf",
   "metadata": {},
   "source": [
    "## Using GPUs to Speed Up Computations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2b7aed9",
   "metadata": {},
   "source": [
    "Things to consider when buying a GPU card:\n",
    "1. Amount of RAM (at least 10GB for image processing / NLP);\n",
    "2. Bandwidth (speed of sending data into and out of GPU);\n",
    "3. Number of cores\n",
    "4. Cooling system\n",
    "\n",
    "https://timdettmers.com/2023/01/30/which-gpu-for-deep-learning/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ad140ea",
   "metadata": {},
   "source": [
    "### Managing the GPU RAM\n",
    "\n",
    "1. Assign different programs specific GPU cores;\n",
    "2. Tell tensroflow to grab only a specific amount of GPU RAM;\n",
    "3. Tell tensorflow to grab memory only when it needs it\n",
    "4. Split a GPU into 2 or more logical devices."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5bf1c16",
   "metadata": {},
   "source": [
    "### Placing Operations and Variables on Devices\n",
    "\n",
    "Best practices:\n",
    "1. Data preprocessing on CPU and neural network operations on the GPU;\n",
    "2. GPUs have fairly limited communication bandwidth, so avoid unnecessary data transfers into and out of the GPUs;\n",
    "3. Addming more CPU RAM is simpler and cheaper, compared to increasing a GPU's RAM, so if a variable is not needed in the next few training steps, then it should probably be placed on the CPU.\n",
    "\n",
    "Note that the CPU is always treated as a single device, even if the machine has multiple CPU cores."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eff079e",
   "metadata": {},
   "source": [
    "### Parallel Execution Across Multiple Devices\n",
    "\n",
    "1. We could train several models in parallel, each on its own GPU setting `CUDA_DEVICE_ORDER` and `CUDA_VISIBLE_DEVICES` so that each script only sees a single GPU device. This is great for hyperparameter tuning as you can train in parallel multiple models with different hyperparameters.\n",
    "2. We can train a model on a single GPU and perform all the preprocessing in parallel on the CPU, using teh dataset's `prefetch()` method to prepare the next few batches in advances so that they are ready when the GPU needs them!\n",
    "3. If the model takes 2 images as input and processes them using 2 CNNs before joining their outputs, then it will probably run much faster if the CNN is placed on a different GPU.\n",
    "4. Create an efficient ensemble, placing different trained model on each GPU so that you can get all the predictions much faster to produce the ensemble's final prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6adcaecd",
   "metadata": {},
   "source": [
    "## Training Models Across Multiple Devices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28ab5ea0",
   "metadata": {},
   "source": [
    "### Model Parallelism (model split across devices)\n",
    "\n",
    "tl;dr May speed up running / training some types of neural networks, and requires special care and tuning!\n",
    "\n",
    "Model is split across the devices (e.g. training a single neural network across multiple devices by chopping the model into separate chunks and running each chunk on a different device). Effectiveness of this model depends on the architecture of the neural network (fully connected networks will not gain much from this approach). \n",
    "\n",
    "For some network architectures such as CNNs, some layers are only partually connected to the lower layers so it's much easier to distribute chunks across devices in an efficient way.\n",
    "\n",
    "Deep recurrent neural networks can also be split a bit more efficiently across multiple GPUs, but it will take time until all the GPUs will be active (splitting horizontally by placing each layer on a different device)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e012ceca",
   "metadata": {},
   "source": [
    "### Data Parallelism / Single Program, Multiple Data (SPMD)\n",
    "\n",
    "Have different devices run the same model but different mini-batches and the gradients are computed by avearging the results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7512ace",
   "metadata": {},
   "source": [
    "#### Using Mirrored Strategy\n",
    "\n",
    "Mirroring all model parameters across all the GPUs and applyng the same parameter updates on every GPU. The tricky part is to efficiently compute the mean of all the gradients from all the GPUs and distribute the result across all the GPUs. This can be done through an *AllReduce* algorithm, a class of algorithms where multiple nodes collaborate to efficiently perform a *reduce operation*, while ensuring that all nodes obtain the same final result."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f19d772e",
   "metadata": {},
   "source": [
    "#### Centralized Parameters\n",
    "\n",
    "Storing the model parameters outside the GPU devices who are performing the computations (*workers*), such as on the CPU. This method allows either synchronous or asynchronous updates.\n",
    "\n",
    "- Synchronous updates, aggregator waits until all gradients are available before it computes the average gradients and passes them to the optimizer, which will update the model parameters. Once a replica has finished computing its gradients, it must wait for the parameters to be updated before it can proceed to the next mini-batch. The downside is that some devices may be slower than others, so fast devices will have to wait for the slow ones at every step (aka slowest device becomes the bottleneck). This can be overcome by ignoring the gradients from the slowest few replicas (typically ~10%).\n",
    "\n",
    "- Asynchronous updates, the gradients are immedately used to update the model parameter whenever a replica is finished (no aggregation step). However, it's even more surprising that they work because there is no guarantee that the computed gradients will still be pointing in the right direction as the current parameter gradients (since the parameters that was used to calculate is most likely already outdated once the gradients is applied, leading to *stale gradients*). Several methods to reduce the effect of stale gradients are\n",
    "    - Reduce the learning rate;\n",
    "    - Drop the stale gradients or scale them down;\n",
    "    - Adjust the mini-batch size\n",
    "    - Start the first few epochs with using just one replica (*warmup phase*) as stale gradients tend to be more damaging at the beginning of training, when gradients are typically large and parameters have not settle into a valley of the cost function yet.\n",
    "\n",
    "Although it seems that synchronous updates with a few spare replicas was more efficient than using asynchronous updates."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "044763ed",
   "metadata": {},
   "source": [
    "#### Bandwidth saturation\n",
    "\n",
    "This happens when adding an extra GPU will not improve performance since the time spent moving the data into and out of the GPU will outweigh the speedup obtained by splitting the computation load, and at that point it will actually slow down training.\n",
    "\n",
    "This issue is more severe for large dense models, since they have a lot of parameters and gradients to transfer.\n",
    "\n",
    "A method to overcome this is via *pipeline parallelism* (combination of model and data parallelism), achieved by chopping the model into consecutive parts (*stages*), each of which is trained on a different machine. This results in an asynchronous pipeline in which all machines work in parallel with very little idle time. During training, each stage alternates one round of forward propagation, and backpropagation: pulling a mini batch from its input queue, processes it, sends the output to the next stage's input queue, then pulls one mini-batch of gradients from its gradient queue, backpropagates these gradients and updates its own model parameters, and pushed the backpropagated gradients to the previous stage's gradient queue. This, however, will face the issue of stale gradients, and is mitigated by having each stage saving the weights during forward propagation and restoring them during backpropagation to ensure that the same weights are used for both forward pass and the backward pass (*weight stashing*)\n",
    "\n",
    "Look up *Pathways* by Google, that uses automated model parallelism, asynchronous gang scheduling and other techniques to reach close to 100% hardware utilization across thousands of TPUs!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0151e022",
   "metadata": {},
   "source": [
    "Realistically, right now, here is what can be done\n",
    "1. Use few powerful GPUs (rather than weak GPUs)\n",
    "2. Group GPUs on a few and very well interconnected servers;\n",
    "3. Drop the float precision (32 bits to 16 buts)\n",
    "4. If centralized parameters is used, shard / split the parameters across multiple parameter servers (adding more parameter servers will reduce the network load on each server and limit the risk of bandwidth saturation)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b31c497",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv(autoencoder_python10) (3.10.11)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
