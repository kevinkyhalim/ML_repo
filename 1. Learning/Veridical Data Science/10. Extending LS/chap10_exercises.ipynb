{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4abb9857",
   "metadata": {},
   "source": [
    "# True or False Exercises\n",
    "\n",
    "## Question 1\n",
    "**Question:**  \n",
    "The magnitude of the LS coefficient of a predictive feature corresponds to how important the feature is for generating the prediction.\n",
    "\n",
    "**Answer:**  \n",
    "\n",
    "---\n",
    "\n",
    "## Question 2\n",
    "**Question:**  \n",
    "The LS algorithm cannot be applied directly to data with categorical predictor variables without modifying the data.\n",
    "\n",
    "**Answer:**  \n",
    "\n",
    "---\n",
    "\n",
    "## Question 3\n",
    "**Question:**  \n",
    "Correlation feature selection can be used for data that has binary predictor variables.\n",
    "\n",
    "**Answer:**  \n",
    "\n",
    "---\n",
    "\n",
    "## Question 4\n",
    "**Question:**  \n",
    "Increasing the number of predictive features in a predictive fit will always improve the predictive performance.\n",
    "\n",
    "**Answer:**  \n",
    "\n",
    "---\n",
    "\n",
    "## Question 5\n",
    "**Question:**  \n",
    "The LS algorithm can only be used to train linear relationships between a response and predictor variable.\n",
    "\n",
    "**Answer:**  \n",
    "\n",
    "---\n",
    "\n",
    "## Question 6\n",
    "**Question:**  \n",
    "The preprocessing judgment calls that you make can affect your predictive performance.\n",
    "\n",
    "**Answer:**  \n",
    "\n",
    "---\n",
    "\n",
    "## Question 7\n",
    "**Question:**  \n",
    "Overfitting happens when your predictive fit captures very specific patterns that exist only in the training data.\n",
    "\n",
    "**Answer:**  \n",
    "\n",
    "---\n",
    "\n",
    "## Question 8\n",
    "**Question:**  \n",
    "The best ridge and lasso regularization hyperparameter is the value that yields the smallest CV error.\n",
    "\n",
    "**Answer:**  \n",
    "\n",
    "---\n",
    "\n",
    "## Question 9\n",
    "**Question:**  \n",
    "More regularization means that regularized coefficients will be closer to the original un-regularized LS coefficients.\n",
    "\n",
    "**Answer:**  \n",
    "\n",
    "---\n",
    "\n",
    "## Question 10\n",
    "**Question:**  \n",
    "It makes more sense to use L2 (ridge) regularization than L1 (lasso) regularization for the LS algorithm because the LS algorithm involves minimizing an L2 (squared) loss function.\n",
    "\n",
    "**Answer:**  \n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfefc19f",
   "metadata": {},
   "source": [
    "# Conceptual Exercises\n",
    "\n",
    "## Question 1\n",
    "**Question:**  \n",
    "Explain why it is common to remove one level of a categorical variable to act as a reference level when creating one-hot encoded variables.\n",
    "\n",
    "**Answer:**  \n",
    "[Your answer here]\n",
    "\n",
    "---\n",
    "\n",
    "## Question 2\n",
    "**Question:**  \n",
    "Define collinearity in the context of prediction problems and discuss why it is an issue.\n",
    "\n",
    "**Answer:**  \n",
    "[Your answer here]\n",
    "\n",
    "---\n",
    "\n",
    "## Question 3\n",
    "**Question:**  \n",
    "Describe one cause of overfitting and describe an analysis that you could conduct to identify whether your predictive fit is overfitting to the training data.\n",
    "**Answer:**  \n",
    "[Your answer here]\n",
    "\n",
    "---\n",
    "\n",
    "## Question 4\n",
    "**Question:**  \n",
    "Explain how lasso can be used for feature selection.\n",
    "\n",
    "**Answer:**  \n",
    "[Your answer here]\n",
    "\n",
    "---\n",
    "\n",
    "## Question 5\n",
    "**Question:**  \n",
    "Describe how ridge regularization stabilizes LS solutions.\n",
    "\n",
    "**Answer:**  \n",
    "[Your answer here]\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb76f4f8",
   "metadata": {},
   "source": [
    "# Mathematical Exercises\n",
    "\n",
    "## Question 1\n",
    "**Question:**  \n",
    "For an exponential relationship y ~ exp(βx), write:  \n",
    "1. Write the format of a reasonable predictive fit (for predicting y based on x) that captures this exponential relationship that could be computed by the LS algorithm.\n",
    "2. Corresponding LS loss function  \n",
    "\n",
    "**Answer:**  \n",
    "[Your answer here]\n",
    "\n",
    "---\n",
    "\n",
    "## Question 2\n",
    "**Question:**  \n",
    "Given linear fit ŷ = b0 + b1 area + b2quality + b3year + b4bedrooms\n",
    "1. Show that the predicted price of a house that has an area of 0 square feet, a quality score of 0, was built in the year 0, and has 0 bedrooms is equal to b0.\n",
    "2. Show that the predicted price of a house will increase by b1 if we increase the living area of the house by a value of 1, without changing the value of the other predictive features. \n",
    "\n",
    "**Answer:**  \n",
    "[Your answer here]\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65ceb632",
   "metadata": {},
   "source": [
    "# Coding Exercises\n",
    "\n",
    "## Question 1\n",
    "**Question:**  \n",
    "In this chapter, we have focused solely on the LS algorithm, but many of the topics introduced in this chapter apply equally well to the LAD algorithm.\n",
    "\n",
    "1. Use the LAD algorithm (e.g., using the lad() function from the “L1pack’’ R package, setting method = \"EM\", or the Python linear_model.LADRegression() class from the sklego library) to generate a predictive fit for house price based on all the available predictive features. You may use the default cleaned/preprocessed version of the data that we used throughout this chapter, or you may use your own. You may want to write your code in a new section at the end of the 05_prediction_ls_adv.qmd (or .ipynb) file in the ames_houses/dslc_documentation/ subfolder of the supplementary GitHub repository.\n",
    "\n",
    "2. Compare the validation set predictive performance of your LAD fit with the corresponding LS (all predictors) fit (e.g., using rMSE, MAE, and correlation).\n",
    "\n",
    "3. Compare the coefficients of your LAD fit with the coefficients of the LS fit.\n",
    "\n",
    "4. Create standardized versions of the LAD and LS coefficients using the bootstrap technique introduced in Section 10.1.2 (i.e., by estimating the SD of the coefficients by retraining the algorithms on bootstrapped training datasets sampled with replacement). Compare the standardized LS and LAD coefficients. Do your conclusions change?\n",
    "\n",
    "5. Assess the stability of your LAD fit (relative to the LS fit) by visualizing the range of predictions generated across (i) data- and (ii) judgment call-perturbations using prediction stability plots. You may want to edit a copy of the relevant code in the 05_prediction_ls_adv.qmd (or .ipynb) file in the ames_houses/dslc_documentation/ subfolder of the supplementary GitHub repository.\n",
    "\n",
    "\n",
    "**Answer:**  \n",
    "[Your answer here]\n",
    "\n",
    "---\n",
    "\n",
    "# Project Exercises\n",
    "\n",
    "## Question 1\n",
    "**Question:**  \n",
    "Predicting happiness (continued) This project extends the happiness prediction project from the previous chapter. The goal will be to create a more sophisticated LS fit than the previous single-predictor fit. The files for this project can be found in the exercises/happiness/ folder on the supplementary GitHub repository.\n",
    "\n",
    "1. In the dslc_documentation/01_cleaning.qmd (or .ipynb) file, conduct a data cleaning and preprocessing examination using all variables in the data. Modify the data cleaning function (in the cleanHappiness.R (or .py) file in the dslc_documentation/functions/ subfolder) and/or write a preprocessing function to preprocess the data (you may want to just have one function that both cleans and preprocesses the data, or separate cleaning and preprocessing functions), and save your functions in relevant .R (or .py) files in the dslc_documentation/functions folder. If you created it in the previous chapter’s exercise, be sure to update the prepareHappiness.R (or .py) file in the dslc_documentation/functions/ subfolder that implements your cleaning/preprocessing procedure. Document any judgment calls that you make (and possible alternatives) in the relevant DSLC documentation files.\n",
    "\n",
    "2. Based on the splitting strategy you used for this project last chapter, split the data into training, validation, and test sets, and create cleaned/preprocessed versions of each dataset (we suggest writing this code in the prepareHappiness.R (or .py) file in the dslc_documentation/functions/ subfolder).\n",
    "\n",
    "3. In the dslc_documentation/02_eda.qmd (or .ipynb) file, conduct an exploratory data analysis (EDA) on the training set, particularly to investigate how a range of different features are related to the happiness score (you can skip this if your EDA from the previous chapter involved all the predictors).\n",
    "\n",
    "4. In the dslc_documentation/03_prediction.qmd (or .ipynb) file, use the LS, ridge, and lasso algorithms to compute a linear predictive fit with multiple features for the happiness score using your pre-processed training data. Be sure to keep a record of all predictive fits that you try (you will be able to filter out poorly performing fits later).\n",
    "\n",
    "5. In the dslc_documentation/03_prediction.qmd (or .ipynb) file, conduct a PCS analysis of your fits, and compare your multipredictor fits’ performance to the single-predictor fit from the previous chapter.\n",
    "\n",
    "6. In the dslc_documentation/03_prediction.qmd (or .ipynb) file, compute standardized versions (using the bootstrap) of the coefficients for your LS fits to identify which variables are most important for predicting happiness.\n",
    "\n",
    "\n",
    "**Answer:**  \n",
    "[Your answer here]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c219151",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
