{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "401d4013",
   "metadata": {},
   "source": [
    "# True or False Exercises\n",
    "\n",
    "## Question 1\n",
    "**Question:**  \n",
    "For any given domain problem, there are many ways that you could define a response variable.\n",
    "\n",
    "**Answer:**  \n",
    "\n",
    "---\n",
    "\n",
    "## Question 2\n",
    "**Question:**  \n",
    "Labeled data is required for training a predictive algorithm, but not for evaluating it.\n",
    "\n",
    "**Answer:**  \n",
    "\n",
    "---\n",
    "\n",
    "## Question 3\n",
    "**Question:**  \n",
    "Creating training, validation, and test datasets using a random split will always be appropriate, no matter the domain problem or future data scenario.\n",
    "\n",
    "**Answer:**  \n",
    "\n",
    "---\n",
    "\n",
    "## Question 4\n",
    "**Question:**  \n",
    "A predictor variable that has a strong predictive relationship with the response variable does not necessarily imply a causal relationship between the predictor and response.\n",
    "\n",
    "**Answer:**  \n",
    "\n",
    "---\n",
    "\n",
    "## Question 5\n",
    "**Question:**  \n",
    "Predictor variable, predictive feature, predictor, and covariate are all different terms used to refer to the same thing (i.e., the variables that are used to generate a prediction of a response).\n",
    "\n",
    "**Answer:**  \n",
    "\n",
    "---\n",
    "\n",
    "## Question 6\n",
    "**Question:**  \n",
    "Cluster membership labels can be thought of as real-world \"ground truth\" response labels for predictive problems.\n",
    "\n",
    "**Answer:**  \n",
    "\n",
    "---\n",
    "\n",
    "## Question 7\n",
    "**Question:**  \n",
    "Collecting more training data observations will always lead to algorithms that produce more accurate predictions.\n",
    "\n",
    "**Answer:**  \n",
    "\n",
    "---\n",
    "\n",
    "## Question 8\n",
    "**Question:**  \n",
    "Collecting additional predictor variables will always lead to more accurate predictions.\n",
    "\n",
    "**Answer:**  \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1513329",
   "metadata": {},
   "source": [
    "\n",
    "# Conceptual Exercises\n",
    "\n",
    "## Question 1\n",
    "**Question:**  \n",
    "Imagine that you work for a search engine company and are tasked with learning about which types of user queries (i.e., phrases that users type into the search engine) result in revenue by developing an algorithm that will predict whether a particular user query will lead to an ad click.\n",
    "\n",
    "1. Suggest a potential continuous response variable\n",
    "2. Suggest a potential binary response variable\n",
    "3. Suggest good predictor variables that might be good predictors of these responses\n",
    "4. How would you evaluate predictability?\n",
    "\n",
    "**Answer:**  \n",
    "\n",
    "---\n",
    "\n",
    "## Question 2\n",
    "**Question:**  \n",
    "You find an algorithm predicting rain using atmospheric data from the past 3 days:\n",
    "1. If trained on data from a city 100 miles away, how would you determine if it's appropriate for your town?\n",
    "2. If trained on your town's data from 10+ years ago, how would you determine if it's appropriate today?\n",
    "\n",
    "**Answer:**  \n",
    "\n",
    "---\n",
    "\n",
    "## Question 3\n",
    "**Question:**  \n",
    "Imagine that you are tasked with developing an algorithm for predicting whether a flight will be delayed using airport flight data collected from flights to and from US airports between 2017 and 2022.\n",
    "1. What are the observational units?\n",
    "2. Define an appropriate response variable (binary/continuous)\n",
    "3. List four predictive features\n",
    "4. What future data would you apply this to?\n",
    "5. How would you split existing data for evaluation?\n",
    "\n",
    "**Answer:**  \n",
    "\n",
    "---\n",
    "\n",
    "## Question 4\n",
    "**Question:**  \n",
    "Identifying Arctic ice cover using static satellite imagery can be a surprisingly challenging problem due to the visual similarity between cloud and ice in satellite images. Imagine that your job is to develop a computational algorithm for predicting whether each pixel of a satellite image taken over an Arctic region corresponds to cloud or ice.\n",
    "\n",
    "Your hypothetical data consists of a single image (covering an area of 100 square kilometers). The image has been converted into a tidy rectangular format, where each row corresponds to a pixel (there are over 100,000 pixels) and the variables/features recorded for each pixel include the grayscale intensity value for the pixel, as well as some more complex features such as the radiance (the brightness detected in a given direction directed toward the sensor) recorded for several angles and wavelengths. Each pixel in the image has been labeled by an expert based on whether it corresponds to cloud or ice.\n",
    "1. What are the observational units?\n",
    "2. Define a response variable (binary/continuous)\n",
    "3. If you only have one image on which to both train and evaluate your algorithm, how would you split your pixels (which come from a single image) into training, validation, and test datasets? Hint: think about whether there is a dependence between the pixels in the image as well as the kind of data to which you would be applying the algorithm.\n",
    "4. If your algorithm can generate accurate predictions for another similar image taken nearby, discuss whether you would feel comfortable using your algorithm trained on this data to predict the locations of cloud and ice across a range of different Arctic regions.\n",
    "\n",
    "**Answer:**  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "349925f4",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
