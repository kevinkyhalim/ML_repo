{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a36837b1",
   "metadata": {},
   "source": [
    "# True or False Exercises\n",
    "\n",
    "## Question 1\n",
    "**Question:**  \n",
    "The technique of SVD is used only for principal component analysis.\n",
    "\n",
    "**Answer:**  \n",
    "FALSE, SVD can be used for other usages too such as image compression, low rank approximation, etc.\n",
    "\n",
    "---\n",
    "\n",
    "## Question 2\n",
    "**Question:**  \n",
    "Any matrix can be decomposed using SVD.\n",
    "\n",
    "**Answer:**  \n",
    "TRUE, SVD can be used to decompose a matrix of any size!\n",
    "\n",
    "---\n",
    "\n",
    "## Question 3\n",
    "**Question:**  \n",
    "The principal component variable loading magnitudes are comparable only if the variables are mean-centered.\n",
    "\n",
    "**Answer:**  \n",
    "FALSE, the are comparable when the variables are SD-scaled, mean centeredness will not have any impact to the component.\n",
    "\n",
    "---\n",
    "\n",
    "## Question 4\n",
    "**Question:**  \n",
    "If principal component analysis is applied to data with positive values, the variable loadings will all be positive too.\n",
    "\n",
    "**Answer:**  \n",
    "FALSE, loadings can be positive and negative, regardless of the values of the data.\n",
    "\n",
    "---\n",
    "\n",
    "## Question 5\n",
    "**Question:**  \n",
    "We are only ever interested in the first principal component.\n",
    "\n",
    "**Answer:**  \n",
    "FALSE, we will be interested in a certain number of principal components based on the extent or total amount of variability that we want the principal components to explain for the original data set!\n",
    "\n",
    "---\n",
    "\n",
    "## Question 6\n",
    "**Question:**  \n",
    "Data cleaning/preprocessing judgment calls can affect the results of principal component analysis.\n",
    "\n",
    "**Answer:**  \n",
    "TRUE, between SD scaling and no SD scaling can directly affect the results of the principal component analysis!\n",
    "\n",
    "---\n",
    "\n",
    "## Question 7\n",
    "**Question:**  \n",
    "Principal component analysis can be applied only to datasets with numeric variables.\n",
    "\n",
    "**Answer:**  \n",
    "TRUE, for non numeric variables, they will have to be converted to numerical (e.g. using one-hot encoding).\n",
    "---\n",
    "\n",
    "## Question 8\n",
    "**Question:**  \n",
    "Principal component analysis only creates summary variables consisting of linear combinations of the original variables.\n",
    "\n",
    "**Answer:**  \n",
    "TRUE\n",
    "\n",
    "---\n",
    "\n",
    "## Question 9\n",
    "**Question:**  \n",
    "The projected data points will exhibit greater variability when projected onto the second principal component than the first principal component.\n",
    "\n",
    "**Answer:**  \n",
    "FALSE, the nth principal component will always have variability greater than the n+1 th principal component!\n",
    "---\n",
    "\n",
    "## Question 10\n",
    "**Question:**  \n",
    "The principal component analysis algorithm requires that the columns in your data have a Gaussian distribution.\n",
    "\n",
    "**Answer:**  \n",
    "FALSE, it is not necessarily a requirement but an assumption. If the underlying data to be transformed has a Gaussian distribution, then the principal components can be more meaningful!\n",
    "---\n",
    "\n",
    "## Question 11\n",
    "**Question:**  \n",
    "A scree plot displays the proportion of variability explained by each principal component.\n",
    "\n",
    "**Answer:**  \n",
    "TRUE!\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04fe9801",
   "metadata": {},
   "source": [
    "# Conceptual Exercises\n",
    "\n",
    "## Question 1\n",
    "**Question:**  \n",
    "Describe, in your own words, what the first principal component is capturing.\n",
    "\n",
    "**Answer:**  \n",
    "The first principal component is a projection of the data that captures the most amount of variability.\n",
    "\n",
    "---\n",
    "\n",
    "## Question 2\n",
    "**Question:**  \n",
    "What information do the matrices V and D contain in the SVD decomposition of a matrix X = $UDV^T$?\n",
    "\n",
    "**Answer:**  \n",
    "V: the loadings of the corresponding  principal component linear combination (size m x m, where m is the number of variables)\n",
    "D: Contains the amount of variability that is explained in each of the principal component\n",
    "\n",
    "---\n",
    "\n",
    "## Question 3\n",
    "**Question:**  \n",
    "Suppose that after performing SVD on a four-dimensional dataset, you obtained a right-singular vector matrix V whose first column is [0.010, 0.003, 0.047, 0.999]. If the original data had variables X1, X2, X3, X4, write the formula for computing the first principal component.\n",
    "\n",
    "**Answer:**  \n",
    "x1 * 0.01 + x2 * 0.003 + x3 * 0.047 + x4 * 0.999\n",
    "---\n",
    "\n",
    "## Question 4\n",
    "**Question:**  \n",
    "Explain why we often want to SD-scale the variables before applying principal component analysis. When might we not want to do this?\n",
    "\n",
    "**Answer:**  \n",
    "We want to SD-scale the variables before applying princiapl component analysis because the data with the highest SD will highly influence the principal component (since the objective of the principal component is to explain the variability of the data as much as possible), hence SD-scaling will avoid the PCA to skew the loadings to the variable with the highest amount of SD / variability.\n",
    "\n",
    "In other words, variance computations (PCA) will yield larger variance values for values with larger number (vice versa) so without SD-scaling, the PCA will unintentionally assume that the variables with larger magnitued contain more variable information!\n",
    "\n",
    "---\n",
    "\n",
    "## Question 5\n",
    "**Question:**  \n",
    "For each of the following projects, discuss whether you think it makes sense to mean-center and SD-scale each of the variables and describe how you would use PCA:\n",
    "\n",
    "1. Your goal is to obtain a single number that you can use to summarize the progression of a patient’s ovarian cancer which is usually determined based on dozens of various measurements from blood tests (including the number of white blood cells, the number of red blood cells, and so on). Each variable in your data is the measurement from a blood test, which corresponds to a count whose minimum value is 0. Each count has a different scale (e.g., we typically have trillions of red blood cells per liter, but only billions of white blood cells).\n",
    "2. Your goal is to develop an algorithm that will predict how much houses will sell for in your city. Your data consists of a large number of numeric features (e.g., the area, quality, and number of bedrooms) for several thousand houses that have recently been sold, and you aim to create a simpler lower-dimensional dataset that you can use as the input for your predictive algorithm.\n",
    "3. You are hoping to learn about public opinion on autonomous vehicles. Your goal is to create a simple visualization, such as a scatterplot or a histogram, that can be used to visualize the different categories of people based on their answers to a survey. The survey is conducted on random members of the public and asks dozens of questions whose answers are each on a scale of 0 to 5, such as “How safe do you think autonomous vehicles are?” and “How much do you know about autonomous vehicle technology?”\n",
    "\n",
    "**Answer:**  \n",
    "1. SD-scale to adjust for the different scale of red blood and white blood cells, but do not need to mean-center since 0 has meaning in this case. We can use the PCA to visualize over time how the values of the first few principal component change over time! In addition, we can also try to create PCs for different components of the blood (i.e. related to red blood cells, white blood cels, thrombocites, etc) and have PCs for each of these large categories to summarize each component!\n",
    "\n",
    "2. SD-scaling and mean-centering should be done. SD-scaling is to ensure that the magnitude of each variable is scaled appropriately (since magnitude of area will be much higher than # of bedrooms), and we would also mean scale because houses don't have the value 0 for the area of their houses. However we should also take note not to sd-scale and mean-center the numeric representations for the categorical variables. We can use the # of PCA up to a certain amount of variability that we want (~80%) and then compare this with the results obtained from using the numerical values themselves. \n",
    "\n",
    "3. neither SD-scale nor mean center because all variables in the survey will have the same scale and the value 0 has meaning in this case. We can use the PCA to summarize the various large categories under each topic of the vehicle (i.e. safetiness, awareness, etc) under 1 single value and we can visualize this using a scatterplot or histogram to see any correlation existing between the different topics.\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f60c666b",
   "metadata": {},
   "source": [
    "# Mathematical Exercises\n",
    "\n",
    "## Question 1\n",
    "**Question:**  \n",
    "If A = [[2.1, 3.5],[-1.2, 7.9]] and B = [[1.6, -5.4],[5.3, 2.0]], compute the product AB manually and verify your computation using python.\n",
    "\n",
    "**Answer:**  \n",
    "[Your answer here]\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5c5c376f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "A = [[2.1, 3.5], [-1.2, 7.9]]\n",
    "B = [[1.6, -5.4], [5.3, 2.0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a5db8356",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[21.91, -4.34],\n",
       "       [39.95, 22.28]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.matmul(A,B)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e7d6247",
   "metadata": {},
   "source": [
    "\n",
    "## Question 2\n",
    "**Question:**  \n",
    "If A = [[2.1, 3.5],[-1.2, 7.9]] is a 2×2 matrix and Y is a column vector = [4, 9], compute the product AY manually and verify your computation with python.\n",
    "\n",
    "**Answer:**  \n",
    "[Your answer here]\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "834fa081",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = [[4], [9]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9dc69013",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[39.9],\n",
       "       [66.3]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.matmul(A, Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4492d28",
   "metadata": {},
   "source": [
    "\n",
    "## Question 3\n",
    "**Question:**  \n",
    "Given singular value matrix Σ = [[30.6],[16.2],[11.2],[6.08]] and right-singular vector matrix V = [[0.45, -0.6, -0.64, 0.15],[-0.4, -0.8, 0.42, -0.17], [0.57, -0.01. 0.23, -0.78], [0.54, -0.08, 0.59, 0.58]]:\n",
    "\n",
    "1. Is it possible to determine how many variables (columns) the original data contained? If so, how many?\n",
    "2. Is it possible to determine how many observations (rows) the original data contained? If so, how many?\n",
    "3. Is it possible to determine how many principal components were computed? If so, how many?\n",
    "4. Compute the proportion of variability explained for each principal component.\n",
    "5. Write a formula (linear combination) for computing each of the first two principal components (using  etc… to represent the original variables).\n",
    "6. Compute the first two principal component values for an observation whose measurements are (0.65, -1.09, 1.21, 0.93).\n",
    "\n",
    "**Answer:**  \n",
    "1. Yes, there are 4 variables\n",
    "2. No, we cannot know this\n",
    "3. Yes, 4 principal components\n",
    "4. See code below\n",
    "5. PC1 = x1 * 0.45 + x2 * -0.4 + x3 * 0.57 + x4 * 0.54\n",
    "PC2 = x1 * -0.6 + x2 * -0.8 + x3 * -0.01 + x4 * -0.08\n",
    "6. See code below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "296e48ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1th Principal Component Variability 0.4775280898876405\n",
      "2th Principal Component Variability 0.25280898876404495\n",
      "3th Principal Component Variability 0.17478152309612982\n",
      "4th Principal Component Variability 0.09488139825218478\n"
     ]
    }
   ],
   "source": [
    "# 4.\n",
    "D = [30.6, 16.2, 11.2, 6.08]\n",
    "\n",
    "for i in range(len(D)):\n",
    "    var = D[i] / np.sum(D)\n",
    "    print(f\"{i+1}th Principal Component Variability {var}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6e0f9bd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.9204 0.3955000000000001\n"
     ]
    }
   ],
   "source": [
    "# 6\n",
    "load1 = [0.45, -0.4, 0.57, 0.54]\n",
    "load2 = [-0.6, -0.8, -0.01, -0.08]\n",
    "obs = [0.65, -1.09, 1.21, 0.93]\n",
    "\n",
    "PC1 = np.matmul(obs, np.transpose(load1))\n",
    "PC2 = np.matmul(obs, np.transpose(load2))\n",
    "print(PC1, PC2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "904aa375",
   "metadata": {},
   "source": [
    "# Coding Exercises\n",
    "\n",
    "## Question 1\n",
    "**Question:**  \n",
    "The code for cleaning and exploring the nutrition data from this chapter can be found in the 01_cleaning.qmd and 02_eda.qmd (or .ipynb) files in the nutrition/dslc_documentation/ subfolder of the supplementary GitHub repository. Run the code and read through the discussion in each of these files and add any other explorations that you wish to conduct. For an advanced version of this exercise, conduct your own cleaning and exploratory analyses of this data\n",
    "**Answer:**  \n",
    "[Your answer here]\n",
    "\n",
    "---\n",
    "\n",
    "## Question 2\n",
    "**Question:**  \n",
    "The code for conducting principal component analysis from this chapter on the nutrition dataset can be found in the 03_pca.qmd (or .ipynb) file in the nutrition/dslc_documentation/ subfolder of the supplementary GitHub repository.\n",
    "\n",
    "1. For the “Major minerals” dataset, compute the correlation between each original “major mineral” nutrient variable and the principal components you computed. Compare these correlations with the variable loadings. The section in the 03_pca.qmd (or .ipynb) file in the nutrition/dslc_documentation/ subfolder is labeled “[Exercise: to complete]”.\n",
    "\n",
    "**Answer:**  \n",
    "[Your answer here]\n",
    "\n",
    "---\n",
    "\n",
    "## Question 3\n",
    "**Question:**  \n",
    "Create a scree plot based on given singular value matrix D = [357.2, 26.2, 12.3, 9.1]. How many PCs would you keep based on the elbow method?\n",
    "\n",
    "**Answer:**  \n",
    "[Your answer here]\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb5419e7",
   "metadata": {},
   "source": [
    "# Project Exercises\n",
    "**Palmer Penguins** The Palmer Penguins dataset contains measurements on size and weight, as well as other characteristics for almost 350 penguins, where each penguin is one of three species (Adelie, Chinstrap, or Gentoo). This data can be loaded into R by installing and loading the palmerpenguins R package (Horst, Hill, and Gorman 2020), and the data has also been provided in the exercises/penguins/data/ folder in the supplementary GitHub repository. There are two alternative versions of the data that you may choose to use for this project: penguins_raw, a version of the data that you will have to clean yourself, and penguins, which is a simplified version of the penguins_raw data that has already been “cleaned” for you (although you are welcome to perform any additional cleaning of this data).\n",
    "\n",
    "The goal of this project is to come up with a simple principal component-based predictive algorithm for predicting the species of any penguin that you meet in the wild (including on a different island than the islands from which the data was originally collected).\n",
    "\n",
    "## Question 1\n",
    "**Question:**  \n",
    "Take 30 percent of the penguins and place them in a validation set. Think about how you want to split up the data in terms of how you will be applying your algorithm. Justify your choice, and summarize it in your DSLC documentation.\n",
    "\n",
    "**Answer:**  \n",
    "[Your answer here]\n",
    "\n",
    "---\n",
    "\n",
    "## Question 2\n",
    "**Question:**  \n",
    "Clean your dataset of choice by creating and completing a 01_clean.qmd (or .ipynb) file, which you may want to place in a dslc_documentation/ subfolder. Be sure to thoroughly document the process (including any judgment calls that you made).\n",
    "**Answer:**  \n",
    "[Your answer here]\n",
    "\n",
    "---\n",
    "\n",
    "## Question 3\n",
    "**Question:**  \n",
    "Conduct an exploratory data analysis of the training data by creating and completing a 02_eda.qmd (or .ipynb) file in the dslc_documentation/ subfolder. Be sure to create at least one explanatory figure.\n",
    "\n",
    "**Answer:**  \n",
    "[Your answer here]\n",
    "\n",
    "---\n",
    "\n",
    "## Question 4\n",
    "**Question:**  \n",
    "Conduct a principal component analysis of the entire training dataset in a file called 03_pca.qmd (or .ipynb) in the dslc_documentation/ subfolder, which you will need to create. Be sure to write down the formula for computing the first principal component in terms of a penguin’s original measurements.\n",
    "\n",
    "**Answer:**  \n",
    "[Your answer here]\n",
    "\n",
    "---\n",
    "\n",
    "## Question 5\n",
    "**Question:**  \n",
    "Visualize the distribution of the values for the first principal component and try to differentiate the values from each penguin species (e.g., using color).\n",
    "\n",
    "**Answer:**  \n",
    "[Your answer here]\n",
    "\n",
    "---\n",
    "\n",
    "## Question 6\n",
    "**Question:**  \n",
    "Come up with some simple thresholding rules for classifying penguins into one of the three species using the first principal component. For example, such a set of rules might be “If PC1 , then classify as Adelie; if  PC1 , then classify as Chinstrap; if PC1 , then classify as Gentoo” (these are just example threshold values—yours will almost certainly be different!)\n",
    "\n",
    "**Answer:**  \n",
    "[Your answer here]\n",
    "\n",
    "---\n",
    "\n",
    "## Question 7\n",
    "**Question:**  \n",
    "Conduct a predictability analysis by estimating the accuracy of your PC1-based thresholding rule based on how many of the withheld validation set penguins’ species are correctly classified using your thresholding rule.\n",
    "\n",
    "**Answer:**  \n",
    "[Your answer here]\n",
    "\n",
    "---\n",
    "\n",
    "## Question 8\n",
    "**Question:**  \n",
    "Conduct a stability analysis by investigating how your results change when you conduct various perturbations throughout the DSLC (including reasonable data perturbations, reasonable data cleaning and preprocessing judgment call perturbations, and reasonable perturbations to any analysis judgment calls that you made).\n",
    "\n",
    "**Answer:**  \n",
    "[Your answer here]\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11f9f911",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.8)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
