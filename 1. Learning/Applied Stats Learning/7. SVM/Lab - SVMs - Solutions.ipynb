{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a6622ee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd  # for easily manipulating databases\n",
    "import numpy as np  # for numerical computations\n",
    "import matplotlib.pyplot as plt  # for plotting\n",
    "import pickle  # pickle is Python's \"serialization\" package -- used to save and load any python object easily"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71fda389",
   "metadata": {},
   "source": [
    "# Lab: Support Vector Machines\n",
    "\n",
    "## The Khan dataset (Gene expression)\n",
    "\n",
    "This week we will work with the Khan dataset, which is a dataset of tumour tissue samples and their gene expression profiles. The covariates are 2,308 gene expression levels, and the targets are a categorical label of tumour type. So we will be exploring (multi-class) classification with SVMs here.\n",
    "\n",
    "We also take this opportunity to introduce to you another useful Python package: Pickle. pickle is Python's \"serialization\" routine, which basically allows you save virtually any Python object (including its current state) and then move it around and load it and pick up where you left off. The dataset here is is saved as a pickle file 'khan_gene.pkl' and you can use the following expression to load it (see the docs if you want to understand further)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3d08c045",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['xtest', 'xtrain', 'ytest', 'ytrain'])\n",
      "      G0001     G0002     G0003     G0004     G0005     G0006     G0007  \\\n",
      "0  0.773344 -2.438405 -0.482562 -2.721135 -1.217058  0.827809  1.342604   \n",
      "1 -0.078178 -2.415754  0.412772 -2.825146 -0.626236  0.054488  1.429498   \n",
      "2 -0.084469 -1.649739 -0.241308 -2.875286 -0.889405 -0.027474  1.159300   \n",
      "3  0.965614 -2.380547  0.625297 -1.741256 -0.845366  0.949687  1.093801   \n",
      "4  0.075664 -1.728785  0.852626  0.272695 -1.841370  0.327936  1.251219   \n",
      "\n",
      "      G0008     G0009     G0010  ...     G2299     G2300     G2301     G2302  \\\n",
      "0  0.057042  0.133569  0.565427  ... -0.238511 -0.027474 -1.660205  0.588231   \n",
      "1 -0.120249  0.456792  0.159053  ... -0.657394 -0.246284 -0.836325 -0.571284   \n",
      "2  0.015676  0.191942  0.496585  ... -0.696352  0.024985 -1.059872 -0.403767   \n",
      "3  0.819736 -0.284620  0.994732  ...  0.259746  0.357115 -1.893128  0.255107   \n",
      "4  0.771450  0.030917  0.278313  ... -0.200404  0.061753 -2.273998 -0.039365   \n",
      "\n",
      "      G2303     G2304     G2305     G2306     G2307     V2308  \n",
      "0 -0.463624 -3.952845 -5.496768 -1.414282 -0.647600 -1.763172  \n",
      "1  0.034788 -2.478130 -3.661264 -1.093923 -1.209320 -0.824395  \n",
      "2 -0.678653 -2.939352 -2.736450 -1.965399 -0.805868 -1.139434  \n",
      "3  0.163309 -1.021929 -2.077843 -1.127629  0.331531 -2.179483  \n",
      "4  0.368801 -2.566551 -1.675044 -1.082050 -0.965218 -1.836966  \n",
      "\n",
      "[5 rows x 2308 columns]\n",
      "0    2\n",
      "1    2\n",
      "2    2\n",
      "3    2\n",
      "4    2\n",
      "Name: Y, dtype: int64\n",
      "(63, 2308)\n",
      "(20, 2308)\n",
      "Y\n",
      "2    23\n",
      "4    20\n",
      "3    12\n",
      "1     8\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "data = pickle.load(open('./khan_gene.pkl', 'rb'))\n",
    "\n",
    "# if you poked around, you would see that its a dictionary. Let's look what's inside.\n",
    "print(data.keys())\n",
    "\n",
    "# looks like a training and testing set. let's look how big it is and also display some of the data\n",
    "print(data['xtrain'].head())  # the entires in the dictionary are Pandas Dataframes\n",
    "print(data['ytrain'].head())\n",
    "print(data['xtrain'].shape)\n",
    "print(data['xtest'].shape)\n",
    "\n",
    "X_train, Y_train = data['xtrain'], data['ytrain']\n",
    "X_test, Y_test = data['xtest'], data['ytest']\n",
    "\n",
    "# explore the different classes\n",
    "print(data['ytrain'].value_counts())  # remember value_counts shows you all the unique values and their frequencies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84dc1713",
   "metadata": {},
   "source": [
    "We see there are 63 examples in the training set and 20 examples in the testing set. We also see there are 4 different tumour classes (the labels). This dataset is the first time we've worked with p>n. In fact, p>>n here! It is always fundamentally easy to overfit in this regime.\n",
    "\n",
    "### Problem 1:\n",
    "\n",
    "Among the kernel functions we discussed in lecture, which should you probably use in this p>>n case and why?\n",
    "\n",
    "### Solutions:\n",
    "We should use the linear kernel, because if the dimensionality is much larger than the number of examples, then there is certainly no need for the additional complexity afforded by other kernels such as the polynomial or radial kernel. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1340358-efa6-4fc7-b100-0b447e97ba66",
   "metadata": {},
   "source": [
    "## SVMs in scikit-learn\n",
    "\n",
    "You can implement a linear SVM in scikit-learn like so. Note there are a few different ways to implement a linear SVM (you can look at the docs (https://scikit-learn.org/stable/modules/svm.html).\n",
    "\n",
    "### Problem 2\n",
    "In lectures, we did not discuss multi-class classification with SVMs much, and like most scikit-learn functions, SVC automatically detects that you need a multi-class classifier and accomodates you. Look at the documentation and determine what method it's using to perform multi-class classificaiton.\n",
    "\n",
    "### Solutions:\n",
    "At the top of this page: https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html#sklearn.svm.SVC, it states that the one-vs-one is used for multi-class support."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4c0604a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "\n",
    "clf = svm.SVC(C=1.0, kernel='linear')\n",
    "clf.fit(X_train, Y_train)  # like all scikit-learn methods, they will automatically infer from the dataset that you are doing multi-class classification\n",
    "print(\"Test set predictions:\", clf.predict(X_test))  # the predictions\n",
    "\n",
    "# let's look at the support vectors\n",
    "sv = clf.support_vectors_\n",
    "print(sv.shape)\n",
    "print(\"So there are %d support vectors\" % sv.shape[0])\n",
    "\n",
    "# have a look at the predicted class labels \n",
    "Y_pred = clf.predict(X_train)\n",
    "print(Y_pred)\n",
    "print(clf.classes_)  # the columns in the matrix correspond to these class labels (ordering is important) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc1f25e7-f746-443b-801f-4fd15cfced51",
   "metadata": {},
   "source": [
    "### Problem 4:\n",
    "\n",
    "Compute the training error and explain the result.\n",
    "\n",
    "### Solutions:\n",
    "The training error is computed like so. It's zero. This is because when p>>n, it is easy to find a hyperplane in this high-dimensional space that can perfectly separate the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a76d8705-2636-4251-ab01-2c49e39621fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training error: 0.0\n"
     ]
    }
   ],
   "source": [
    "# the training error would be fit like so\n",
    "Y_train_fit = clf.predict(X_train)\n",
    "train_error = np.mean((Y_train - Y_train_fit)**2)  # this is MSE\n",
    "print(\"Training error:\", train_error)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90c2ca16",
   "metadata": {},
   "source": [
    "## Comparison Experiment\n",
    "\n",
    "When p>>n, you need to do aggressive feature selection. What models should you immediately start thinking about? Recall that models like Lasso and random forests perform aggressive feature selection. But this is classification -- so what model is analogous to Lasso? The natural choice is logistic regression with an L1 penalty. \n",
    "\n",
    "We will prepare to run a comparative experiment.\n",
    "\n",
    "First, pool the observations into one dataset like so. The following code says to concatenate the DataFrames like X_train and X_test along the first axis. Importantly, we should tell Pandas to ignore the index labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a6ecac74-5337-45b9-a5f2-cf374837b842",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      G0001     G0002     G0003     G0004     G0005     G0006     G0007  \\\n",
      "0  0.773344 -2.438405 -0.482562 -2.721135 -1.217058  0.827809  1.342604   \n",
      "1 -0.078178 -2.415754  0.412772 -2.825146 -0.626236  0.054488  1.429498   \n",
      "2 -0.084469 -1.649739 -0.241308 -2.875286 -0.889405 -0.027474  1.159300   \n",
      "3  0.965614 -2.380547  0.625297 -1.741256 -0.845366  0.949687  1.093801   \n",
      "4  0.075664 -1.728785  0.852626  0.272695 -1.841370  0.327936  1.251219   \n",
      "\n",
      "      G0008     G0009     G0010  ...     G2299     G2300     G2301     G2302  \\\n",
      "0  0.057042  0.133569  0.565427  ... -0.238511 -0.027474 -1.660205  0.588231   \n",
      "1 -0.120249  0.456792  0.159053  ... -0.657394 -0.246284 -0.836325 -0.571284   \n",
      "2  0.015676  0.191942  0.496585  ... -0.696352  0.024985 -1.059872 -0.403767   \n",
      "3  0.819736 -0.284620  0.994732  ...  0.259746  0.357115 -1.893128  0.255107   \n",
      "4  0.771450  0.030917  0.278313  ... -0.200404  0.061753 -2.273998 -0.039365   \n",
      "\n",
      "      G2303     G2304     G2305     G2306     G2307     V2308  \n",
      "0 -0.463624 -3.952845 -5.496768 -1.414282 -0.647600 -1.763172  \n",
      "1  0.034788 -2.478130 -3.661264 -1.093923 -1.209320 -0.824395  \n",
      "2 -0.678653 -2.939352 -2.736450 -1.965399 -0.805868 -1.139434  \n",
      "3  0.163309 -1.021929 -2.077843 -1.127629  0.331531 -2.179483  \n",
      "4  0.368801 -2.566551 -1.675044 -1.082050 -0.965218 -1.836966  \n",
      "\n",
      "[5 rows x 2308 columns]\n",
      "0    2\n",
      "1    2\n",
      "2    2\n",
      "3    2\n",
      "4    2\n",
      "Name: Y, dtype: int64\n",
      "(83, 2308)\n",
      "(83,)\n"
     ]
    }
   ],
   "source": [
    "X = pd.concat([X_train, X_test], axis=0, ignore_index=True)\n",
    "Y = pd.concat([Y_train, Y_test], axis=0, ignore_index=True)\n",
    "\n",
    "print(X.head())\n",
    "print(Y.head())\n",
    "\n",
    "print(X.shape)\n",
    "print(Y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba9db095-001e-4188-9072-4b7e24bc42d0",
   "metadata": {},
   "source": [
    "### Prepare for an experiment\n",
    "\n",
    "If you did an exploration of the data, you'd see that all predictors are numeric valued, their values are all within the same range, and there are no missing values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "c84c64a0-0591-4518-a895-f39e7b0c3ae3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "float64    2308\n",
      "Name: count, dtype: int64\n",
      "0\n",
      "           G0001      G0002      G0003      G0004      G0005      G0006  \\\n",
      "count  83.000000  83.000000  83.000000  83.000000  83.000000  83.000000   \n",
      "mean    0.095508  -1.661939  -0.203707  -1.359657  -1.376694   0.355879   \n",
      "std     0.904378   0.882391   0.704871   1.174159   0.585545   0.780256   \n",
      "min    -2.683846  -3.007805  -1.851509  -3.366796  -3.216379  -1.263723   \n",
      "25%    -0.265028  -2.322890  -0.568117  -2.277448  -1.806496  -0.065349   \n",
      "50%     0.239410  -1.813392  -0.103030  -1.736136  -1.266558   0.519032   \n",
      "75%     0.735390  -1.147199   0.290426   0.113641  -0.933310   0.878176   \n",
      "max     1.471485   0.654770   1.160742   0.583779  -0.264747   2.452728   \n",
      "\n",
      "           G0007      G0008      G0009      G0010  ...      G2299      G2300  \\\n",
      "count  83.000000  83.000000  83.000000  83.000000  ...  83.000000  83.000000   \n",
      "mean    1.681210   0.137365   0.241915   0.393151  ...  -0.459341  -0.575929   \n",
      "std     0.443620   0.497517   0.451735   0.509725  ...   0.686443   0.658340   \n",
      "min     0.776063  -1.218072  -0.639227  -1.572142  ...  -2.276917  -2.226550   \n",
      "25%     1.400180  -0.266450  -0.051030   0.061377  ...  -0.887540  -1.013634   \n",
      "50%     1.603541   0.194250   0.191942   0.470379  ...  -0.307477  -0.543693   \n",
      "75%     1.932492   0.461451   0.548549   0.722011  ...  -0.004870  -0.132281   \n",
      "max     2.946642   1.224011   1.437866   1.169443  ...   1.077559   1.281323   \n",
      "\n",
      "           G2301      G2302      G2303      G2304      G2305      G2306  \\\n",
      "count  83.000000  83.000000  83.000000  83.000000  83.000000  83.000000   \n",
      "mean   -1.326569  -0.742486  -0.633767  -1.777800  -1.673760  -0.967169   \n",
      "std     0.691523   0.553185   1.095169   0.937735   0.875732   0.614945   \n",
      "min    -2.847312  -2.273998  -3.294138  -4.803621  -5.496768  -3.146555   \n",
      "25%    -1.853447  -1.038884  -1.433326  -2.331722  -2.127933  -1.365390   \n",
      "50%    -1.253163  -0.731680  -0.454918  -1.586698  -1.680397  -1.064211   \n",
      "75%    -0.813623  -0.412507   0.286692  -1.189754  -1.062350  -0.623558   \n",
      "max     0.210180   0.588231   1.040171   0.059118   0.385874   0.479335   \n",
      "\n",
      "           G2307      V2308  \n",
      "count  83.000000  83.000000  \n",
      "mean   -0.510991  -1.454831  \n",
      "std     0.593329   0.629243  \n",
      "min    -2.691193  -3.110021  \n",
      "25%    -0.834146  -1.896223  \n",
      "50%    -0.428939  -1.393923  \n",
      "75%    -0.146951  -0.988490  \n",
      "max     0.570471   0.041142  \n",
      "\n",
      "[8 rows x 2308 columns]\n",
      "Y\n",
      "2    29\n",
      "4    25\n",
      "3    18\n",
      "1    11\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(X.dtypes.value_counts())  # all predictors are numeric valued\n",
    "print(X.isnull().sum().sum())  # there are no missing values\n",
    "\n",
    "print(X.describe())  # eyeball descriptive summaries of the data\n",
    "\n",
    "# In classification, it's also a good idea to look at the \"balance\" of the labels in the dataset\n",
    "print(Y.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe21e6ff-29a0-4696-b2f8-9a4acfffc106",
   "metadata": {},
   "source": [
    "## Experiment\n",
    "\n",
    "Now run the experiment as we have done in previous labs to compare random forest classifiers, gradient boosting classifiers, and the (linear) SVC. Note that logistic regression with an L1 penalty takes a bit too long to run on this huge p dataset, so you can leave it out of this exercise; for the same reason, you may wish to skip cross-validating over the GBT parameters.\n",
    "\n",
    "### Problem 5\n",
    "What is a good naive baseline to use? Think of one and include in the comparison.\n",
    "\n",
    "Additionally, what performance metric should you use for this classification problem? There are several choices, and in my opinion AUC is still the most popular and respected choice. However, using AUC here involves several concepts and data cleaning/management steps that are cumbersome for the purposes of this lab, so we will instead simply use accuracy (which should be too bad since the dataset does not appear to be severely imbalanced).\n",
    "\n",
    "Finally, because this dataset has a small number of training examples, we should probably not randomly subsample train/test splits like we have done in previous labs. We should use something like K-fold splitting to ensure we maximally use up all the available data.\n",
    "\n",
    "The following code snippet below is copy and pasted from previous labs, where I have already applied the use of the accuracy score and K-fold train/test splits for you. Modify it to your needs.\n",
    "\n",
    "### Solutions:\n",
    "In the below code, I use the majority class appearing in Y_train as the naive baseline. The predicted class proability for every test case with this naive baseline is the fraction of training datapoints belonging to this majority class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "1831400a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N train: 74\n",
      "N test: 9\n",
      "Finished RF split: 1\n",
      "Finished GBT split: 1\n",
      "Finished SVM split: 1\n",
      "This split took: 14.22 secs\n",
      "N train: 74\n",
      "N test: 9\n",
      "Finished RF split: 2\n",
      "Finished GBT split: 2\n",
      "Finished SVM split: 2\n",
      "This split took: 14.05 secs\n",
      "N train: 74\n",
      "N test: 9\n",
      "Finished RF split: 3\n",
      "Finished GBT split: 3\n",
      "Finished SVM split: 3\n",
      "This split took: 14.50 secs\n",
      "N train: 75\n",
      "N test: 8\n",
      "Finished RF split: 4\n",
      "Finished GBT split: 4\n",
      "Finished SVM split: 4\n",
      "This split took: 14.81 secs\n",
      "N train: 75\n",
      "N test: 8\n",
      "Finished RF split: 5\n",
      "Finished GBT split: 5\n",
      "Finished SVM split: 5\n",
      "This split took: 14.70 secs\n",
      "N train: 75\n",
      "N test: 8\n",
      "Finished RF split: 6\n",
      "Finished GBT split: 6\n",
      "Finished SVM split: 6\n",
      "This split took: 13.81 secs\n",
      "N train: 75\n",
      "N test: 8\n",
      "Finished RF split: 7\n",
      "Finished GBT split: 7\n",
      "Finished SVM split: 7\n",
      "This split took: 15.01 secs\n",
      "N train: 75\n",
      "N test: 8\n",
      "Finished RF split: 8\n",
      "Finished GBT split: 8\n",
      "Finished SVM split: 8\n",
      "This split took: 14.80 secs\n",
      "N train: 75\n",
      "N test: 8\n",
      "Finished RF split: 9\n",
      "Finished GBT split: 9\n",
      "Finished SVM split: 9\n",
      "This split took: 14.44 secs\n",
      "N train: 75\n",
      "N test: 8\n",
      "Finished RF split: 10\n",
      "Finished GBT split: 10\n",
      "Finished SVM split: 10\n",
      "This split took: 14.76 secs\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegressionCV  # logistic regression with built in regularization. The penalty parameter is Cross-validated\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import KFold\n",
    "import matplotlib.pyplot as plt # for plotting\n",
    "import time # for timekeeping\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# The following grids will be used to cross-validate over the GBT and SVM parameters\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "param_grid_gbt = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'learning_rate': [0.01, 0.05, 0.1],\n",
    "}\n",
    "\n",
    "param_grid_svm = {\n",
    "    'C': [0.001, 0.01, 0.1, 1, 10, 100],\n",
    "}\n",
    "\n",
    "# container for the AUC scores\n",
    "scores = {}  # this time we will use a dictionary structure to contain the performance metrics\n",
    "for model_name in ['rf', 'gbt', 'svm']:\n",
    "    scores[model_name] = []  # initialize this dictionary entry to an empty list\n",
    "\n",
    "kf = KFold(n_splits=10, shuffle=True)   # will make a test set size of about 10% of the data \n",
    "\n",
    "i_ = 0\n",
    "for train_inds, test_inds in kf.split(X):\n",
    "    \n",
    "    t_start = time.time()  # used to time things\n",
    "    i_ += 1\n",
    "\n",
    "    print(\"N train:\", len(train_inds))\n",
    "    print(\"N test:\", len(test_inds))\n",
    "\n",
    "    X_train = X.iloc[train_inds, :].copy()  # remember .iloc is used for integer indexing!\n",
    "    X_test = X.iloc[test_inds, :].copy()\n",
    "    \n",
    "    Y_train = Y.iloc[train_inds]\n",
    "    Y_test = Y.iloc[test_inds]\n",
    "    \n",
    "    # standardize the predictors\n",
    "    for key in X_train.columns:\n",
    "        x_mean = X_train[key].mean()\n",
    "        x_std = X_train[key].std()\n",
    "        X_train.loc[:, key] = (X_train[key] - x_mean) / x_std  # vectorized\n",
    "        X_test.loc[:, key] = (X_test[key] - x_mean) / x_std  # use the training statistics to transform the test cases\n",
    "\n",
    "    # # logistic regression with L1 penalty cross-validated\n",
    "    # clf = LogisticRegressionCV(penalty='l1', solver='saga', max_iter=10000)  # uses 5-fold cross validation by default\n",
    "    # clf.fit(X_train, Y_train)  \n",
    "    # Y_pred = clf.predict_proba(X_test)  \n",
    "    # auc = roc_auc_score(Y_test, Y_pred, multi_class='ovr')  # one-vs-rest scheme for multi-class classification\n",
    "    # auc_scores['logreg_l1'].append(auc)\n",
    "    # print(\"Finished logreg l1 split:\", i_)\n",
    "\n",
    "    # Random Forest Classifier\n",
    "    clf = RandomForestClassifier(n_estimators=500)\n",
    "    clf.fit(X_train, Y_train) \n",
    "    Y_pred = clf.predict(X_test)\n",
    "    score = accuracy_score(Y_test, Y_pred)\n",
    "    scores['rf'].append(score)\n",
    "    print(\"Finished RF split:\", i_)\n",
    "\n",
    "    # Gradient Boosting Classifier\n",
    "    clf = GradientBoostingClassifier(n_estimators=500)\n",
    "    clf.fit(X_train, Y_train)\n",
    "    Y_pred = clf.predict(X_test)\n",
    "    score = accuracy_score(Y_test, Y_pred)\n",
    "    scores['gbt'].append(score)\n",
    "    print(\"Finished GBT split:\", i_)\n",
    "\n",
    "    # SVM\n",
    "    grid = GridSearchCV(\n",
    "        svm.SVC(kernel='linear'),\n",
    "        param_grid_svm,\n",
    "        cv=5,\n",
    "    )\n",
    "    grid.fit(X_train, Y_train)\n",
    "    clf = grid.best_estimator_\n",
    "    clf.fit(X_train, Y_train) \n",
    "    Y_pred = clf.predict(X_test)\n",
    "    score = accuracy_score(Y_test, Y_pred)\n",
    "    scores['svm'].append(score)\n",
    "    print(\"Finished SVM split:\", i_)\n",
    "    \n",
    "    t_elapsed = time.time() - t_start\n",
    "    print(\"This split took: %.2f secs\" % t_elapsed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "61dbe089-ecc9-4188-894b-76f1e524d400",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logreg_l1 ave accuracy: nan\n",
      "rf ave accuracy: 1.0\n",
      "gbt ave accuracy: 0.9\n",
      "svm ave accuracy: 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/koa/anaconda3/lib/python3.10/site-packages/numpy/core/fromnumeric.py:3504: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/Users/koa/anaconda3/lib/python3.10/site-packages/numpy/core/_methods.py:129: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'Accuracy')"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_names = list(scores.keys())\n",
    "\n",
    "for name in model_names:\n",
    "    scores_ = scores[name]\n",
    "    print(\"%s ave accuracy:\" % name, np.mean(scores_))\n",
    "\n",
    "plt.boxplot([scores[name] for name in model_names])\n",
    "plt.xticks(list(range(1, len(model_names) + 1)), model_names)\n",
    "plt.ylabel('Accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "001d7ca4-6b83-4ac2-a820-bd8b9a8cbe18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'logreg_l1': [],\n",
       " 'rf': [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0],\n",
       " 'gbt': [1.0, 1.0, 1.0, 1.0, 0.875, 0.625, 0.75, 1.0, 0.75, 1.0],\n",
       " 'svm': [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]}"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a10f6548-18b7-41e8-96c0-39f0dbaa7759",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
